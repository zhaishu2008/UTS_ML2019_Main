{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": false,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": true
    },
    "colab": {
      "name": "NB03_NeuralNets.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "wVUTU9rjUCa6",
        "Zs6JSG2dUCa9",
        "JO5I6XORUCbb",
        "q8U9gwapUCbh",
        "GBSmMDUNUCb0",
        "svbJbpJ1UCb3",
        "yGh3huCtUCb7"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/junjy007/UTS_ML2019_Main/blob/master/NB03_NeuralNets.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3w52ilutUCa5",
        "colab_type": "text"
      },
      "source": [
        "# 1 Basic Neural Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rh48fEPwUCa6",
        "colab_type": "text"
      },
      "source": [
        "## 1.1 Prepare Environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wVUTU9rjUCa6",
        "colab_type": "text"
      },
      "source": [
        "### 1.1.1 Library Import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FRXDSL7fUCa7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import math\n",
        "import torch\n",
        "import plotly.express as px\n",
        "import pandas as pd\n",
        "import random"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "heading_collapsed": true,
        "id": "Zs6JSG2dUCa9",
        "colab_type": "text"
      },
      "source": [
        "### 1.1.2 Visualisation Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "heading_collapsed": true,
        "hidden": true,
        "id": "lAxot3-VUCa-",
        "colab_type": "text"
      },
      "source": [
        "#### A Simple visualiser"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "fhnum9e5UCa-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def simple_visualise(mod):\n",
        "    xx, yy = np.meshgrid(np.arange(-5, 5.01, 0.05), \n",
        "                         np.arange(-5, 5.01, 0.05))\n",
        "    xx = xx.flatten()\n",
        "    yy = yy.flatten()\n",
        "    d = pd.DataFrame(data=dict(x=xx, y=yy, \n",
        "                               z=[mod.forward((x, y)) \n",
        "                                  for x, y in zip(xx, yy)]))\n",
        "    fig = px.scatter(d, x='x', y='y', color='z')\n",
        "    fig.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4wg130tRUCbA",
        "colab_type": "text"
      },
      "source": [
        "## 1.2 Neural Networks Model Definition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WN8l4UDyUCbB",
        "colab_type": "text"
      },
      "source": [
        "### 1.2.1 Definition and Naive Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gbuCeexkUCbB",
        "colab_type": "text"
      },
      "source": [
        "Let us literally translate the definition of a neural network into computer implementation:\n",
        "Neural network: Multiple _layers_ of _perceptron(s)_.\n",
        "```python\n",
        "def compute_neural_network(x):\n",
        "    # 0. prepare the input for the first layer\n",
        "    layer_input = x\n",
        "    for layer_idx in [0, 1, 2]:\n",
        "        # 1. fill output of this layer by executing each\n",
        "        #    perceptron in this layer\n",
        "        layer_output = []\n",
        "        for perceptron in net_layers[layer_idx]:\n",
        "            perceptron.compute_output(layer_input) \n",
        "            # Note all perceptrons in this layer share the same\n",
        "            # `layer_input`\n",
        "        #!!----------------------------------    \n",
        "        # 2. pass the output of THIS layer\n",
        "        #    to the NEXT layer as the input\n",
        "        #------------------------------------\n",
        "        layer_input = layer_output\n",
        "        # END OF LOOP OVER `layer_idx`\n",
        "```\n",
        "Recall that a perception is to get the weighted sum of all attributes in an input, followed by some _activation_. See below:\n",
        "```python\n",
        "def compute_perceptron(x):\n",
        "    weighted_sum = sum([xi * wi for xi, wi in zip(x, weights)])\n",
        "    return activation_function(weighted_sum)\n",
        "```\n",
        "Of course, we will need to get the weights and the activation function setup. So we will use an object class to represent both the perceptrons and the networks.\n",
        "\n",
        "NB: If you don't understand the construction `[t for t in list_of_t_values]`, please checkout tutorials about Python list."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "code_folding": [],
        "id": "Z1QMYZACUCbC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sigmoid_func(a):\n",
        "    return 1 / (1 + math.exp(-a))\n",
        "\n",
        "\n",
        "\n",
        "class Perceptron2D:\n",
        "    \"\"\"Perceptron model: linearly combine data attributes followed by a non-linear activation\n",
        "    This is a simplified implementation and deals with data with 2 attributes. \n",
        "    You can also refer to the more complete implementation in the note of Week 3.\n",
        "    \"\"\"\n",
        "    def __init__(self, w0=1, w1=0, activation_func=sigmoid_func):\n",
        "        self.w0 = w0\n",
        "        self.w1 = w1\n",
        "        self.act = activation_func\n",
        "    \n",
        "    def forward(self, x):\n",
        "        wsum = x[0] * self.w0 + x[1] * self.w1\n",
        "        sigmoid_wsum = self.act(wsum) # sigmoid for activation\n",
        "        return sigmoid_wsum\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GODTXZ5XUCbE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Let's have a look at how the perceptron worked on 2D data\n",
        "p = Perceptron2D(0.5, -2.5)\n",
        "simple_visualise(p)\n",
        "# Please note the effect of activation by examining the z-value."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cUJOhBWpUCbJ",
        "colab_type": "text"
      },
      "source": [
        "__EXERCISE__\n",
        "\n",
        "In the code cell above, adjust the model parameters and observe the change of the model behaviour."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CyIwfxtaUCbK",
        "colab_type": "text"
      },
      "source": [
        "__EXERCISE__\n",
        "\n",
        "Use a different activation function. Such as\n",
        "$$\n",
        "\\begin{align}\n",
        "y(h) = \\left\\{ \\begin{array}{c}\n",
        "0, \\textrm{ if } h \\leq 0 \\\\\n",
        "h, \\textrm{ if } h > 0\n",
        "\\end{array} \\right.\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "Add implement your activation like:\n",
        "```python\n",
        "def relu_func(h):\n",
        "    # compute y\n",
        "    # HINT: consider using `max`\n",
        "    return y\n",
        "```\n",
        "Then use your function to construct a Perceptron check the behaviour of the perceptron."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cJWmN3evUCbK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def my_activation_func(h):\n",
        "    return max(0, h)\n",
        "p = Perceptron2D(0.1, -0.5, activation_func=my_activation_func)\n",
        "simple_visualise(p)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tTP1EbFhUCbN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def k2(x):\n",
        "    return [xi**2 for xi in x]\n",
        "\n",
        "class Perceptron2DX:\n",
        "    \"\"\"Perceptron model wrapped. We transform the input before\n",
        "    processing them using the perceptron.\n",
        "    \"\"\"\n",
        "    def __init__(self, percep, xtransform=k2):\n",
        "        self.perceptron = percep\n",
        "        self.xtransform = xtransform\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return self.perceptron.forward(self.xtransform(x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s00Np9AYUCbO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pp = Perceptron2DX(p)\n",
        "simple_visualise(pp)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EPBoMCovUCbR",
        "colab_type": "text"
      },
      "source": [
        "__EXERCISE__\n",
        "\n",
        "__1__:\n",
        "Use a different transform function. Such as\n",
        "$$\n",
        "\\begin{align}\n",
        "x'_1 = \\sin(\\omega_1 \\cdot x_1) \\\\\n",
        "x'_2 = \\cos(\\omega_2 \\cdot x_2)\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "__2__:\n",
        "Using your transform function on a _different perceptron core_, e.g.\n",
        "```python\n",
        "vanilla_perceptron_2 = Perceptron2D(\n",
        "        -1, 1, \n",
        "        activation_func=my_activation_func)\n",
        "pp2a = Perceptron2DX(vanilla_perceptron_2, \n",
        "                     xtransform=new_transform)\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "Yi6j8xVFUCbS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def new_transform(x):\n",
        "    tx0 = math.sin(x[0] * 5)\n",
        "    tx1 = math.cos(x[1])\n",
        "    return [tx0, tx1]\n",
        "\n",
        "pp2 = Perceptron2DX(p, xtransform=new_transform)\n",
        "simple_visualise(pp2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "eXNpW_lfUCbU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ly0_p0 = Perceptron2D(+1, -1, activation_func=my_activation_func)\n",
        "ly0_p1 = Perceptron2D(-1, -3, activation_func=my_activation_func)\n",
        "ly1_p = Perceptron2D(-1, 0.5, activation_func=math.tanh)\n",
        "def new_transform(x):\n",
        "    tx0 = ly0_p0.forward(x)\n",
        "    tx1 = ly0_p1.forward(x)\n",
        "    return [tx0, tx1]\n",
        "\n",
        "pp3 = Perceptron2DX(ly1_p, xtransform=new_transform)\n",
        "simple_visualise(pp3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WT9U35ihUCbX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Try yourself: can you change the parameters so \n",
        "ly0_p0 = Perceptron2D(+1, -1, activation_func=lambda x:x)\n",
        "ly0_p1 = Perceptron2D(-1, -3, activation_func=lambda x:x)\n",
        "ly1_p = Perceptron2D(-1, 0.5, activation_func=math.tanh)\n",
        "def new_transform(x):\n",
        "    tx0 = ly0_p0.forward(x)\n",
        "    tx1 = ly0_p1.forward(x)\n",
        "    return [tx0, tx1]\n",
        "\n",
        "pp3 = Perceptron2DX(ly1_p, xtransform=new_transform)\n",
        "simple_visualise(pp3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9m0DJ9TmUCba",
        "colab_type": "text"
      },
      "source": [
        "### 1.2.2 Multiple Layer Perceptron"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZpM4hZ9mUCbb",
        "colab_type": "text"
      },
      "source": [
        "Alternatively (to the nested perceptrons above), we can define an NeuralNet class to hold all perceptrons in all layers. The advantage is that now we can easily extend the network to have more layers. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JO5I6XORUCbb",
        "colab_type": "text"
      },
      "source": [
        "#### Naive Implementation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "code_folding": [],
        "id": "QgrxLsCjUCbc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define a class, so our network can manage its \"perceptrons\" easily\n",
        "class NeuralNet:\n",
        "    \"\"\"NeuralNet represents a simple neural network object class.\n",
        "    As an example, it consists of 2 layers of perceptrons. \n",
        "    The first layer has 2 perceptrons and the second one has 1.\n",
        "    \n",
        "    The perceptrons deal with data of 2 attributes.\n",
        "    \"\"\"\n",
        "    def __init__(self, perc0_w=(-1, 1), perc1_w=(2, -1), perc2_w=(0.5, 0.5)):\n",
        "        self.layers = [[Perceptron2D(*perc0_w), Perceptron2D(*perc1_w)], \n",
        "                       [Perceptron2D(*perc2_w)]] # *(w0,w1) expand the values in tuple\n",
        "        \n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Compute the network output layer by layer. \"forward\" is a conventional\n",
        "        term for execute computing of a net.\n",
        "        \"\"\"\n",
        "        # use layer-0 to process x and get what's to feed to layer-1\n",
        "        layer1_input = [p.forward(x) for p in self.layers[0]]\n",
        "        # get the final output from layer-1\n",
        "        final_output = [p.forward(layer1_input) for p in self.layers[1]]\n",
        "        # note we have only two layers, so I didn't use a loop over the layers\n",
        "        return final_output[0]\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3FRvnnjLUCbe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "net0 = NeuralNet()\n",
        "simple_visualise(net0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6LBwem7sUCbg",
        "colab_type": "text"
      },
      "source": [
        "__EXERCISE__ (optional, similar to one above)\n",
        "\n",
        "Adjust parameters to show how the network behaviour changes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q8U9gwapUCbh",
        "colab_type": "text"
      },
      "source": [
        "#### Computation using Matrix Operations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5hc5Kf4WUCbh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def matmul(A, B):\n",
        "    \"\"\"\n",
        "    matmul computs A x B for two matrices\n",
        "    :param A: a collective object contains rows of a matrix\n",
        "        A[i], i-th row, another collective object contains the elements\n",
        "        A[i][j], the element\n",
        "    :param B: similar to A\n",
        "    \"\"\"\n",
        "    # figure out the size of A and B and the result\n",
        "    rows = len(A)\n",
        "    elements_inner = len(A[0])\n",
        "    assert elements_inner == len(B), \"Rows of B must be the same as Cols of A\"\n",
        "    cols = len(B[0])\n",
        "    # Initialise C to the appropriate size\n",
        "    C = [[0.0 for ci in range(cols)] for ri in range(rows)]\n",
        "    \n",
        "    # Fill C: 2 outer loops are for each element\n",
        "    for r in range(rows):\n",
        "        for c in range(cols):\n",
        "            # Compute element [i][j]\n",
        "            for k in range(elements_inner):\n",
        "                C[r][c] += A[r][k] * B[k][c]\n",
        "    return C"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hq8IOPczUCbj",
        "colab_type": "text"
      },
      "source": [
        "HINT: read the code and try it while watching the accompany video."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TIYdsnCtUCbj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define a class, so our network can manage its \"perceptrons\" easily\n",
        "class NeuralNetV2:\n",
        "    \"\"\"NeuralNetV2 represents a simple neural network object class.\n",
        "    This object will manage all the neurons in the network. \n",
        "    \"\"\"\n",
        "    def __init__(self, neuron_numbers_in_layers=[2, 2, 1],\n",
        "                 weights=None):\n",
        "        \"\"\"\n",
        "        :param neuron_numbers_in_layers: first/last -- inputs and outputs\n",
        "        :param weights: a dict, weights[\"in0out1\"] represents the weights\n",
        "          for computing layer1 from layer0, if layer0 has 3 inputs and layer1\n",
        "          has 2 outputs, then the weight will be a matrix of 3 x 2, i.e.\n",
        "          weights[\"in0out1\"][i][j] is the weight for computing element-j in layer1\n",
        "          by using element-i in layer0.\n",
        "        \"\"\"\n",
        "        # Weights between \n",
        "        # Layer1 and 2, Layer 2 and 3, ...\n",
        "        \n",
        "        self.weights = dict()\n",
        "        self.activ_fn = dict()\n",
        "        self.neuron_nums = neuron_numbers_in_layers\n",
        "        self.layer_num = len(neuron_numbers_in_layers) - 1\n",
        "        \n",
        "        # DEFINE (!not DO!, we dont have input X now) computation between layers\n",
        "        for l_in in range(self.layer_num):\n",
        "            l_out = l_in + 1\n",
        "            pkey = f\"in{l_in}out{l_out}\"\n",
        "            try:\n",
        "                # try to use provided weight\n",
        "                W = weights[pkey] # NOTE: should copy\n",
        "            except:\n",
        "                # if not provided ...\n",
        "                n_in = neuron_numbers_in_layers[l_in]\n",
        "                n_out = neuron_numbers_in_layers[l_out]\n",
        "                W = [[random.gauss(0, 0.1) for j in range(n_out)] \n",
        "                     for i in range(n_in)] # see init above\n",
        "            self.weights[pkey] = W\n",
        "            self.activ_fn[pkey] = math.tanh # you may want to try your own\n",
        "\n",
        "        \n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        DO computations:\n",
        "        \n",
        "        Compute the network output layer by layer. \"forward\" is a conventional\n",
        "        term for execute computing of a net.\n",
        "        :param x: a list of list: x[i], sample-i, having a number of input attributes\n",
        "          if you have only one sample, input it as \n",
        "              [[0, 1, 0]], \n",
        "              NOT [0, 1, 0]\n",
        "        \"\"\"\n",
        "        layer_input = x\n",
        "        for l_in in range(self.layer_num):\n",
        "            # Use layer-in to process x and get what's to feed to layer-out\n",
        "            # Setup \n",
        "            l_out = l_in + 1\n",
        "            pkey = f\"in{l_in}out{l_out}\"\n",
        "            # Compute the weighted sum \n",
        "            layer_pre_activation = matmul(layer_input, self.weights[pkey])\n",
        "            # Perform activation(the construction below is equivalent to nested loops)\n",
        "            layer_out = list(map(lambda x_:list(map(self.activ_fn[pkey], x_)), \n",
        "                            layer_pre_activation))\n",
        "            layer_input = layer_out # feed the output of this layer to the next layer\n",
        "        return layer_out\n",
        "    \n",
        "    \n",
        "class VisWrap:\n",
        "    \"Wrap I/O for the new network object for visualisation\"\n",
        "    def __init__(self, nn):\n",
        "        self.nn = nn\n",
        "        \n",
        "    def forward(self, x):\n",
        "        return self.nn.forward([x])[0][0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jYS7V1skUCbk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nn2 = NeuralNetV2()\n",
        "nn2_wrap = VisWrap(nn2)\n",
        "simple_visualise(nn2_wrap)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R9LkqT8fUCbo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Let's try to adjust the weights. Carefully keep the number of weights \n",
        "# consistent with the number of neurons you had set to the layers.\n",
        "nn2 = NeuralNetV2(\n",
        "    neuron_numbers_in_layers=[2, 3, 1],\n",
        "    weights={\"in0out1\":[[0, 0.5, 1], [-1, -5, 2]],\n",
        "             \"in1out2\":[[-1], [+1], [0.5]]})\n",
        "nn2.activ_fn[\"in1out2\"] = lambda x:max(0, x)\n",
        "nn2_wrap = VisWrap(nn2)\n",
        "simple_visualise(nn2_wrap)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "obHRTA-LUCbr",
        "colab_type": "text"
      },
      "source": [
        "## 1.3 Training Neural Nets via Backprop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PoM3OtvlUCbs",
        "colab_type": "text"
      },
      "source": [
        "It is not trivial to come up with a simple rule to adjust all the parameters in the entire neural network stucture (recall when we consider a single perceptron, we did propose intuitive scheme to improve the fitness of the model to data). \n",
        "\n",
        "The idea is to take a divide-and-conquer scheme. Let's take a careful look at the computation in one perceptron. Throw the machine learning terminology in wind and focus on the computation steps only.\n",
        "$$\n",
        "\\begin{align}\n",
        "a &\\leftarrow w_0 \\cdot x_0 + w_1 \\cdot x_1 \\\\\n",
        "h &\\leftarrow g(a) \\\\\n",
        "Loss &\\leftarrow Compare(h, y)\n",
        "\\end{align}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ad2TNJ2dUCbt",
        "colab_type": "text"
      },
      "source": [
        "Let us think about the statement \"to make the loss smaller\" with a bit care: which specific means we could possibily take to \"make\" the final loss change? During training, we change the model parameters, including $w_{i,j}$. So we need to know the influence on the final loss of each model parameter. In this section we will examine an example of so-called \"backpropagation\" process."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mqe4e5ZBUCbu",
        "colab_type": "text"
      },
      "source": [
        "### 1.3.1 Adjust Parameters to Modify Model Behaviour"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ISZfJ4-HUCbu",
        "colab_type": "text"
      },
      "source": [
        "Given training data $\\{(x_1, y_1), (x_2, y_2), \\dots\\}$, we would like the net to predict for each $x_i$ the target value $y_i$. If this is the case, then our mission has completed. Of course, this is generally NOT the case if we start from a random set of model parameters.\n",
        "\n",
        "For example, if we have one training sample $(x=(2.5, 2), y=1.0)$, let us compare the prediction given by the network above and the target value $y=1.0$: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z4snoSyCUCbu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# First, check the current output of the net\n",
        "def sigm(x):\n",
        "    return 1/(1+math.exp(-x))\n",
        "\n",
        "nn2 = NeuralNetV2(\n",
        "    neuron_numbers_in_layers=[2, 3, 1],\n",
        "    weights={\"in0out1\":[[0, 0.5, 1], [-1, -5, 2]],\n",
        "             \"in1out2\":[[-1], [+1], [0.5]]})\n",
        "nn2.activ_fn[\"in0out1\"] = sigm\n",
        "nn2.activ_fn[\"in1out2\"] = sigm\n",
        "\n",
        "nn2_wrap = VisWrap(nn2)\n",
        "simple_visualise(nn2_wrap)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_DrB23dUCbx",
        "colab_type": "text"
      },
      "source": [
        "Let's check the nets behaviour at one data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b6B6nQhHUCbx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nn2.forward([(2.5, 2)])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OZUBpvC5UCbz",
        "colab_type": "text"
      },
      "source": [
        "This is smaller than the desired output 1.0. So we want the output to increase at this $x$. Let's learn how to adjust the network using gradients computed through backprop."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tsgt_1wgUCb0",
        "colab_type": "text"
      },
      "source": [
        "### 1.3.2 Manual Backprop Through Layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EBSCj4bcUCb0",
        "colab_type": "text"
      },
      "source": [
        "Let us implement the backpropagation for a three layer simple net. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "heading_collapsed": true,
        "id": "GBSmMDUNUCb0",
        "colab_type": "text"
      },
      "source": [
        "#### Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "G7tCzam4UCb1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##############################################################\n",
        "# HELPER FUNCTIONS\n",
        "# You do NOT need to learn those to USE modern neural networks\n",
        "# Those functions provide basic array functions in LOW efficiency\n",
        "# but clear manner. You may want to check them if you want to\n",
        "# UNDERSTAND the technical details of NN.\n",
        "# --------\n",
        "# First define sigmoid gradient\n",
        "import math\n",
        "def sigm(x): # redefine here for reference\n",
        "    return 1/(1+math.exp(-x))\n",
        "\n",
        "def gsigmoid(x):\n",
        "    return math.exp(-x)/(1+math.exp(-x))**2\n",
        "\n",
        "def elementwise_apply(f, nested_list):\n",
        "    return list(map(lambda x_:list(map(f, x_)), nested_list))\n",
        "\n",
        "def elementwise_times(nested_list1, nested_list2):\n",
        "    return [[a * b for a, b in zip(r1, r2)] \n",
        "            for r1, r2 in zip(nested_list1, nested_list2)]\n",
        "\n",
        "def mat_tr(nested_list):\n",
        "    return [c for c in zip(*nested_list)]\n",
        "\n",
        "def shape(nested_list):\n",
        "    return len(nested_list), len(nested_list[0])\n",
        "##############################################################"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "HvEZwbwcUCb2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# UNIT Test: gsigmoid -- Test the others.\n",
        "test_x = [-3, -1, 0, 1, 3, 5]\n",
        "test_eps = 1e-4\n",
        "for x_ in test_x:\n",
        "    numerical_diff = (sigm(x_ + test_eps) - sigm(x_)) / test_eps\n",
        "    analytic_diff = gsigmoid(x_)\n",
        "    print(f\"NumDiff: {numerical_diff:.3f} ~ AnaDiff: {analytic_diff:.3f}\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "svbJbpJ1UCb3",
        "colab_type": "text"
      },
      "source": [
        "#### Backprop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4FAax_r8UCb4",
        "colab_type": "text"
      },
      "source": [
        "Below I explicily write out the forward method followed by a backward computation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_mUDgRuaUCb4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def special_forward(nn, x):\n",
        "    \"\"\"\n",
        "    This is a special version for manual implementing and testing the backpropagation algorithm. \n",
        "    We only use the network \n",
        "    We don't use the computation and activation of network `nn` \n",
        "    \"\"\"\n",
        "    \n",
        "    # Copy-and-paste and simplify forward computation here:\n",
        "    layer1_input = x\n",
        "    layer1_pre_activation = matmul(layer1_input, nn.weights[\"in0out1\"])\n",
        "    layer1_out = elementwise_apply(sigm, layer1_pre_activation)\n",
        "    \n",
        "    layer2_input = layer1_out # feed the output of this layer to the next layer\n",
        "    \n",
        "    layer2_pre_activation = matmul(layer2_input, nn.weights[\"in1out2\"])\n",
        "    layer2_out = elementwise_apply(sigm, layer2_pre_activation)\n",
        "\n",
        "    layer2_pre_activation_g = elementwise_apply(gsigmoid, layer2_pre_activation)\n",
        "    w12_g = matmul(mat_tr(layer2_input), layer2_pre_activation_g)\n",
        "    layer1_out_g = matmul(layer2_pre_activation_g, mat_tr(nn.weights[\"in1out2\"]))\n",
        "    layer1_pre_activation_g = elementwise_times(\n",
        "        elementwise_apply(gsigmoid, layer1_pre_activation),\n",
        "        layer1_out_g)\n",
        "    w01_g = matmul(mat_tr(layer1_input), layer1_pre_activation_g)\n",
        "\n",
        "    return layer2_out, w01_g, w12_g\n",
        "    \n",
        "# TODO: Mark this somewhere else: this Les Mis is FUNNY! https://www.youtube.com/watch?v=dF495ERjRUo"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Hohj248UCb5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "out, w01_g, w12_g = special_forward(nn2, [[2.5, 2]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yGh3huCtUCb7",
        "colab_type": "text"
      },
      "source": [
        "#### Numerical verification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OE5A0QRzUCb7",
        "colab_type": "text"
      },
      "source": [
        "Next, let us check element by element how does our backprop work. The plan is to adjust each adjustable parameter a bit and check the change of the final output."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MGcWdVXwUCb8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# test adjusting weights in0out1\n",
        "wkey = \"in0out1\"\n",
        "w_rows, w_cols = shape(nn2.weights[wkey])\n",
        "numerical_g = [[0 for c in range(w_cols)] for r in range(w_rows)]\n",
        "test_eps = 1e-4\n",
        "test_x = [[2.5, 2]]\n",
        "old_out = nn2.forward(test_x)\n",
        "\n",
        "for r in range(w_rows):\n",
        "    for c in range(w_cols):\n",
        "        old_value = nn2.weights[wkey][r][c] # save the old value to put back after test        \n",
        "        nn2.weights[wkey][r][c] += test_eps\n",
        "        new_out = nn2.forward(test_x)\n",
        "        diff = new_out[0][0] - old_out[0][0] # check the effect of adjusting the corresponding parameter\n",
        "        numerical_g[r][c] = diff / test_eps\n",
        "        # put the old value back\n",
        "        nn2.weights[wkey][r][c] = old_value\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1RkoCqkIUCb9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"numerical differential\")\n",
        "print(numerical_g)\n",
        "print(\"analytical differential\")\n",
        "print(w01_g)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P0DgWKygUCb-",
        "colab_type": "text"
      },
      "source": [
        "__EXECISE__ [Optional]\n",
        "\n",
        "1. Read the code and Explain what you had observed.\n",
        "2. Check the computation for weights transforms data from layer 1 to 2.\n",
        "3. Integrate the backpropagation function into the network class.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y0cg1cTUUCb_",
        "colab_type": "text"
      },
      "source": [
        "### 1.3.3 Using Computational Framework"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ysVa4MDTUCb_",
        "colab_type": "text"
      },
      "source": [
        "Modern framework allows us to easily perform all the steps above. The example above can be reformulated as"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NS8l1zsYUCb_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2XVWg7U5UCcA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MyNN(nn.Module):\n",
        "    def __init__(self, neuron_numbers_in_layers=[2, 3, 1]):\n",
        "        super(MyNN, self).__init__()\n",
        "        \n",
        "        self.layers = nn.ModuleList(\n",
        "            [nn.Linear(in_features=nin, out_features=nout)\n",
        "             for nin, nout in zip(neuron_numbers_in_layers[:-1], \n",
        "                                  neuron_numbers_in_layers[1:])])\n",
        "        \n",
        "    def forward(self, x):\n",
        "        h = x\n",
        "        for l in self.layers:\n",
        "            h = torch.tanh(l(h))\n",
        "        return h\n",
        "    \n",
        "class TorchVisWrap:\n",
        "    def __init__(self, nn):\n",
        "        self.nn = nn\n",
        "    def forward(self, x):\n",
        "        y = self.nn(torch.Tensor(x).unsqueeze(dim=0))\n",
        "        return y.item()\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y-7qONAUUCcC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.manual_seed(42)\n",
        "nn3 = MyNN([2, 6, 1])\n",
        "nn3_wrap = TorchVisWrap(nn3)\n",
        "simple_visualise(nn3_wrap)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9WdZnca_UCcG",
        "colab_type": "text"
      },
      "source": [
        "Let us perform training, call it changing a neural network behaviour or searching in the hypotheses space of neural networks, it is up to your viewpoint. Eg. We want the net to generate\n",
        "    + for (4, -4)\n",
        "    - for (4, 4)\n",
        "    + for (-4, 4)\n",
        "    - for (-4, -4)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q0K9PtCKUCcG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.optim import Adam\n",
        "optim = Adam(nn3.parameters(), lr=0.01) # manager: adjust params according to grads"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jRTdCbaVUCcH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_steps = 50\n",
        "visualise_every_n_steps = 10\n",
        "trn_X = torch.Tensor([[4, -4], [4, 4], [-4, 4], [-4, -4]])\n",
        "trn_y = torch.Tensor([[1.0], [-1.0], [1.0], [-1.0]])\n",
        "for it in range(train_steps):\n",
        "    loss = ((trn_y - nn3(trn_X))**2).sum()\n",
        "    optim.zero_grad() # reset gradients (to clear computed gradients from previous steps)\n",
        "    loss.backward() # In one stroke, all gradients are computed!\n",
        "    optim.step()  # apply the gradients to the parameters\n",
        "    if it % visualise_every_n_steps == 0:\n",
        "        # Check the effect\n",
        "        simple_visualise(nn3_wrap)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o6aqq8Q_OwSr",
        "colab_type": "text"
      },
      "source": [
        "# 2 Deep NN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "62kdUcZIRE4f",
        "colab_type": "text"
      },
      "source": [
        "## 2.0 Prepare Environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QpVmM4E9MZmf",
        "colab_type": "text"
      },
      "source": [
        "### Libraries and Helpers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J1Yfv7M2MhD0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as fn\n",
        "from torchvision.datasets import MNIST, CIFAR10\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "from torch.utils.data.dataset import Subset\n",
        "from torchvision.transforms import Compose, ToTensor, Normalize, ToPILImage\n",
        "from torch.optim import Adam, SGD\n",
        "mnist_dataset_trn = MNIST(root='./data', download=True, train=True,\n",
        "                          transform=ToTensor())\n",
        "mnist_dataset_tst = MNIST(root='./data', download=True, train=False,\n",
        "                          transform=ToTensor())\n",
        "mnist_train_loader = DataLoader(mnist_dataset_trn, batch_size=4)\n",
        "mnist_test_loader = DataLoader(mnist_dataset_tst, batch_size=4, shuffle=True)\n",
        "mnist_examples = mnist_dataset_trn[0]\n",
        "device = torch.device(\"cuda:0\") if torch.cuda.is_available \\\n",
        "    else torch.device(\"cpu\")\n",
        "\n",
        "def evaluate_model(model, tst_dataloader, max_iter=None):\n",
        "    total_corr = 0\n",
        "    total_num = 0\n",
        "    model.eval()\n",
        "    for i, (x, y) in enumerate(tst_dataloader):\n",
        "        with torch.no_grad():\n",
        "            pred_class_p = model(x.to(device))\n",
        "        corr_num = (torch.argmax(pred_class_p, dim=1).cpu() == y).sum()\n",
        "        total_corr += corr_num.item()\n",
        "        total_num += len(y)\n",
        "        if max_iter is not None:\n",
        "            if i >= max_iter:\n",
        "                break\n",
        "    accu = total_corr / total_num\n",
        "    return accu\n",
        "\n",
        "def simple_logger_factory(tst_dataloader=None):\n",
        "    def logger(model, info):\n",
        "        print(f\"Epoch {info['epoch']}, Iter {info['iter']}, \" +\n",
        "            f\"Train Loss {info['loss']}\")\n",
        "    def logger_eval(model, info):\n",
        "        test_accu = evaluate_model(model, tst_dataloader)\n",
        "        print(f\"Epoch {info['epoch']}, Iter {info['iter']}, \" +\n",
        "            f\"Train Loss {info['loss']} Test Accuracy {test_accu}\")\n",
        "    return logger if tst_dataloader is None else logger_eval\n",
        "\n",
        "simple_mnist_logger = simple_logger_factory(mnist_test_loader)\n",
        "    \n",
        "def train_model(model, trn_dataloader, start_iter=0,\n",
        "                max_iters=None, max_epoches=1, \n",
        "                trainer_class=Adam,\n",
        "                evaluate_every_n_iters=1000,\n",
        "                save_check_point_every_n_iters=10000,\n",
        "                evaluate_callback_fn=simple_logger_factory(),\n",
        "                **kwargs):\n",
        "    optim = trainer_class(model.parameters(), **kwargs)\n",
        "    model.train()\n",
        "    epoches = 0\n",
        "    iters = start_iter\n",
        "    running_loss = 0\n",
        "    while True:\n",
        "        for i, (x, y) in enumerate(trn_dataloader):\n",
        "            optim.zero_grad()\n",
        "            pred_class_p = model(x.to(device))\n",
        "            loss = fn.nll_loss(pred_class_p, y.to(device))\n",
        "            loss.backward()\n",
        "            optim.step()\n",
        "\n",
        "            iters += 1\n",
        "            running_loss += loss.item() \n",
        "            if iters % evaluate_every_n_iters == 0:\n",
        "                evaluate_callback_fn(\n",
        "                    model, dict(epoch=epoches, iter=iters, \n",
        "                                loss=running_loss / evaluate_every_n_iters))\n",
        "                running_loss = 0\n",
        "            if max_iters is not None and iters >= max_iters:\n",
        "                break\n",
        "\n",
        "        epoches += 1\n",
        "        if epoches >= max_epoches:\n",
        "            break\n",
        "        \n",
        "    return\n",
        "\n",
        "def count_param_num(model):\n",
        "    n = 0\n",
        "    for p in model.parameters():\n",
        "        n += p.numel()\n",
        "    return n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "io13HbPWKvX1",
        "colab_type": "text"
      },
      "source": [
        "## 2.1 Elements of Deep Neural Networks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NuTdwf_pK2z-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Example of classifying hand-written digits.\n",
        "x, y = mnist_dataset_trn[42]\n",
        "print(f\"An example of digit {y}\")\n",
        "ToPILImage()(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03GP-XBAsqVi",
        "colab_type": "text"
      },
      "source": [
        "### 2.1.1 Shallow Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AiUvQ6lERzA4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# A \"shallow\" network\n",
        "class Net1(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net1, self).__init__()\n",
        "        self.lin1 = nn.Linear(in_features=28*28, out_features=64)\n",
        "        self.lin2 = nn.Linear(in_features=64, out_features=10)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        n = x.shape[0]\n",
        "        h = fn.relu(self.lin1(x.view(n, -1)))\n",
        "        y = fn.log_softmax(self.lin2(h), dim=1)\n",
        "        return y\n",
        "nn1 = Net1().to(device)\n",
        "train_model(nn1, mnist_train_loader, evaluate_callback_fn=simple_mnist_logger)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7shEN2Bts0Lj"
      },
      "source": [
        "### 2.1.2 Go Deeper"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "AzEvgAmQs0Lm",
        "colab": {}
      },
      "source": [
        "# A naive deep network\n",
        "class NetD(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NetD, self).__init__()\n",
        "        self.lin1 = nn.Linear(in_features=28*28, out_features=64)\n",
        "        self.lin2 = nn.Linear(in_features=64, out_features=64)\n",
        "        self.lin3 = nn.Linear(in_features=64, out_features=64)\n",
        "        self.lin4 = nn.Linear(in_features=64, out_features=64)\n",
        "        self.lin5 = nn.Linear(in_features=64, out_features=10)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        n = x.shape[0]\n",
        "        h = fn.relu(self.lin1(x.view(n, -1)))\n",
        "        h = fn.relu(self.lin2(h))\n",
        "        h = fn.relu(self.lin3(h))\n",
        "        h = fn.relu(self.lin4(h))\n",
        "        h = self.lin5(h)\n",
        "        y = fn.log_softmax(h, dim=1)\n",
        "        return y\n",
        "dnn1 = NetD().to(device)\n",
        "train_model(dnn1, mnist_train_loader, evaluate_callback_fn=simple_mnist_logger)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "R83yfC35ujmN"
      },
      "source": [
        "### 2.1.3 Essentials: Convolutional Net"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3oSIXeynujmQ",
        "colab": {}
      },
      "source": [
        "# A naive deep network\n",
        "class NetCD(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NetCD, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3)\n",
        "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3)\n",
        "        self.conv3 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3)\n",
        "        self.lin4 = nn.Linear(in_features=32*3*3, out_features=64)\n",
        "        self.lin5 = nn.Linear(in_features=64, out_features=10)\n",
        "        \n",
        "    \n",
        "    def forward(self, x):\n",
        "        n = x.shape[0]\n",
        "        h = fn.max_pool2d(fn.relu(self.conv1(x)), kernel_size=2)\n",
        "        h = fn.max_pool2d(fn.relu(self.conv2(h)), kernel_size=2)\n",
        "        h = fn.relu(self.conv3(h)).view(n, -1)\n",
        "        h = fn.relu(self.lin4(h))\n",
        "        h = self.lin5(h)\n",
        "        y = fn.log_softmax(h, dim=1)\n",
        "        return y\n",
        "\n",
        "dnn2 = NetCD().to(device)\n",
        "train_model(dnn2, mnist_train_loader, evaluate_callback_fn=simple_mnist_logger)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jyhrGOGkvzNb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "count_param_num(dnn2), count_param_num(dnn1), count_param_num(nn1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5oURQPW91Z9S"
      },
      "source": [
        "### 2.1.4 Essentials: Direct Link"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "v0jKDtKY1Z9U",
        "colab": {}
      },
      "source": [
        "# A naive deep network\n",
        "class NetCDR(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NetCDR, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3)\n",
        "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=16, kernel_size=3, padding=1)\n",
        "        self.conv3 = nn.Conv2d(in_channels=16, out_channels=16, kernel_size=3, padding=1)\n",
        "        self.lin4 = nn.Linear(in_features=576, out_features=64)\n",
        "        self.lin5 = nn.Linear(in_features=64, out_features=10)\n",
        "        \n",
        "    \n",
        "    def forward(self, x):\n",
        "        n = x.shape[0]\n",
        "        h = fn.max_pool2d(fn.relu(self.conv1(x)), kernel_size=2)\n",
        "        h = fn.max_pool2d(fn.relu(self.conv2(h)) + h , kernel_size=2)\n",
        "        h = (fn.relu(self.conv3(h)) + h).view(n, -1)\n",
        "        h = fn.relu(self.lin4(h))\n",
        "        h = self.lin5(h)\n",
        "        y = fn.log_softmax(h, dim=1)\n",
        "        return y\n",
        "\n",
        "dnn3 = NetCDR().to(device)\n",
        "train_model(dnn3, mnist_train_loader, evaluate_callback_fn=simple_mnist_logger)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "416FAJSM4szG"
      },
      "source": [
        "### 2.1.5 Essentials: Optimiser"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i2CYLhHBhETA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nn1a = Net1().to(device)\n",
        "train_model(nn1a, mnist_train_loader, evaluate_every_n_iters=100, \n",
        "            max_iters=2000,\n",
        "            trainer_class=Adam, lr=0.0001)\n",
        "\n",
        "nn1b = Net1().to(device)\n",
        "train_model(nn1b, mnist_train_loader, evaluate_every_n_iters=100, \n",
        "            max_iters=2000,\n",
        "            trainer_class=SGD, lr=0.0001) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "vtkCPCzh85fz"
      },
      "source": [
        "### 2.1.6 Essentials: Mini-batches"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bBK2elSe85f0",
        "colab": {}
      },
      "source": [
        "nn1a = Net1().to(device)\n",
        "mnist_train_loader128 = DataLoader(mnist_dataset_trn, batch_size=128)\n",
        "train_model(nn1a, mnist_train_loader128, evaluate_every_n_iters=10, \n",
        "            max_iters=100, trainer_class=Adam, lr=0.0001)\n",
        "\n",
        "nn1b = Net1().to(device)\n",
        "mnist_train_loader4 = DataLoader(mnist_dataset_trn, batch_size=4)\n",
        "train_model(nn1b, mnist_train_loader128, evaluate_every_n_iters=80, \n",
        "            max_iters=800, trainer_class=Adam, lr=0.0001)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "kDcbA2g37nF2"
      },
      "source": [
        "### 2.1.7 Essentials: Batch Normalisation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_0ays4KH7nF5",
        "colab": {}
      },
      "source": [
        "# A naive deep network\n",
        "class NetCDRB(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NetCDRB, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3)\n",
        "        self.bn12 = nn.BatchNorm2d(16)\n",
        "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=16, kernel_size=3, padding=1)\n",
        "        self.bn23 = nn.BatchNorm2d(16)\n",
        "        self.conv3 = nn.Conv2d(in_channels=16, out_channels=16, kernel_size=3, padding=1)\n",
        "        self.bn34 = nn.BatchNorm2d(16)\n",
        "        self.lin4 = nn.Linear(in_features=576, out_features=64)\n",
        "        self.lin5 = nn.Linear(in_features=64, out_features=10)\n",
        "        \n",
        "    \n",
        "    def forward(self, x):\n",
        "        n = x.shape[0]\n",
        "        h = fn.max_pool2d(fn.relu(self.conv1(x)), kernel_size=2)\n",
        "        h = self.bn12(h)\n",
        "        h = fn.max_pool2d(fn.relu(self.conv2(h)) + h , kernel_size=2)\n",
        "        h = self.bn23(h)\n",
        "        h = self.bn34(fn.relu(self.conv3(h)) + h).view(n, -1)\n",
        "        h = fn.relu(self.lin4(h))\n",
        "        h = self.lin5(h)\n",
        "        y = fn.log_softmax(h, dim=1)\n",
        "        return y\n",
        "\n",
        "dnn4 = NetCDRB().to(device)\n",
        "train_model(dnn4, mnist_train_loader, evaluate_callback_fn=simple_mnist_logger)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "R4n7dCkr-dYo"
      },
      "source": [
        "### 2.1.7 Essentials: Regularisation by Dropout"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TilhLJYg-dYr",
        "colab": {}
      },
      "source": [
        "# A naive deep network\n",
        "class NetCDRD(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NetCDRD, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3)\n",
        "        self.dropout1 = nn.Dropout2d(inplace=True)\n",
        "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=16, kernel_size=3, padding=1)\n",
        "        self.dropout2 = nn.Dropout2d(inplace=True)\n",
        "        self.conv3 = nn.Conv2d(in_channels=16, out_channels=16, kernel_size=3, padding=1)\n",
        "        self.lin4 = nn.Linear(in_features=576, out_features=64)\n",
        "        self.lin5 = nn.Linear(in_features=64, out_features=10)\n",
        "        \n",
        "    \n",
        "    def forward(self, x):\n",
        "        n = x.shape[0]\n",
        "        h = fn.max_pool2d(fn.relu(self.conv1(x)), kernel_size=2)\n",
        "        h = self.dropout1(h)\n",
        "        h = fn.max_pool2d(fn.relu(self.conv2(h)) + h , kernel_size=2)\n",
        "        h = self.dropout2(h)\n",
        "        h = (fn.relu(self.conv3(h)) + h).view(n, -1)\n",
        "        h = fn.relu(self.lin4(h))\n",
        "        h = self.lin5(h)\n",
        "        y = fn.log_softmax(h, dim=1)\n",
        "        return y\n",
        "dnn5_dp = NetCDRD().to(device)\n",
        "\n",
        "small_train = Subset(mnist_dataset_trn, torch.arange(600))\n",
        "train_loader_sm = DataLoader(small_train, batch_size=4)\n",
        "train_model(dnn5_dp, train_loader_sm, evaluate_callback_fn=simple_mnist_logger,\n",
        "            evaluate_every_n_iters=150, max_epoches=10)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5xCY-8YENMV3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dnn5_nodp = NetCDR().to(device)\n",
        "train_model(dnn5_nodp, train_loader_sm, evaluate_callback_fn=simple_mnist_logger,\n",
        "            evaluate_every_n_iters=150, max_epoches=10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_evAuXycOwSr",
        "colab_type": "text"
      },
      "source": [
        "## 2.2 Automatic Differential Architecture Builder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80iOyHdoOwSv",
        "colab_type": "text"
      },
      "source": [
        "### 2.2.1 Managed variable and operator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nu-coCbnOwSw",
        "colab_type": "text"
      },
      "source": [
        "Modern architectures allow the user to  describe the computations in a data model (e.g. neural networks) naturally, and automatically configure necessary steps and the data structures for back propagation.  To be concrete, in the simple “processing step”:\n",
        "```\n",
        "Y = X1 + X2\n",
        "```\n",
        "The user has actually describe a relationship that “the value of a variable Y is the sum of the values of variables X1 and X2”.  Put it into details, \n",
        "> `Y`  is the result of an `operation of adding` two numbers\n",
        "\n",
        "> The `operation` takes its input 1 from `X1`;  takes its input 2 from `X2`\n",
        "\n",
        "Describing this seemingly simple process in details seems to be tautology. The advantage is that now it is possible to automatically construct the backpropagation chain to compute how the changes in the input X1 and X2 will affect the output Y. \n",
        "\n",
        "Let us represent the _variables_ and _operators_ we will use in _tractable_ computations. \n",
        "\n",
        "__Variables__: \n",
        "- _contain_ the following information:\n",
        "    - `data`: the content value\n",
        "    - `grad`: its effect on one target value, say, $\\frac{\\partial L}{\\partial x}$\n",
        "    - `from_op`: \"ancester\" of this piece of information, `None` or some operator. E.g. in $y = x_1 + x_2$, $x_1$ and $x_2$ have none ancester, $y$ is `from_op`erator \"+\".\n",
        "    - `id_`: and unique global ID\n",
        "- _perform_ the following functions:\n",
        "    - applying operator to `self` (and `another` `variable` object), see `__add__` function below.\n",
        "    - backpropating information of `grad` to the `from_op` (if it exists)\n",
        "\n",
        "__Operators__:\n",
        "- _contain_:\n",
        "    - a\n",
        "- _perform_ the following functions:\n",
        "    - applyi\n",
        "    \n",
        "    \n",
        "__EXERCISE__: Examine the operation of `Y = X1 + X2`\n",
        "\n",
        "__REMARK__\n",
        "You may think one target is too limited, but ..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "code_folding": [],
        "id": "iWYWrbN3OwSx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NeuData:\n",
        "    d_count = 0 # A class-wise *global* counter\n",
        "    \n",
        "    def __init__(self, data=0, grad=0, from_op=None, name=None):\n",
        "        # Essential information\n",
        "        # 1. the \"content\" of this variable\n",
        "        self.data = data \n",
        "        # 2. how this variable affects the \"total objective\"\n",
        "        # of a system. For machine learning problems, the\n",
        "        # system is determined by a global evaluation, often\n",
        "        # called a \"loss\"\n",
        "        self.grad = grad\n",
        "        # 3. the operation from which this variable has been computed\n",
        "        # if this variable is given (no computation is needed)\n",
        "        # this is None\n",
        "        self.from_op = from_op \n",
        "        \n",
        "        # Comp. Graph maintainance (and diagnosis and visualisation)\n",
        "        self.id_ = f\"D{NeuData.d_count}\"\n",
        "        NeuData.d_count += 1 # a global serial number\n",
        "\n",
        "        # Further information for diagnosis and visualisation\n",
        "        self.name = self.id_ if name is None else name\n",
        "        \n",
        "        # If configured, output log information\n",
        "        # Optionally, we can output some report at this point.\n",
        "        # E.g.\n",
        "        print(f\"Variable {self.name} has been created\" + \n",
        "              (\".\" if self.from_op is None else \n",
        "              f\" from the result of {self.from_op.name}\"))\n",
        "        \n",
        "    def __add__(self, another):\n",
        "        \"\"\"\n",
        "        This function builds a \"computational structure\" performing\n",
        "        ADDING between two Variables. \n",
        "        \n",
        "        The method __add__ will be called when the following piece\n",
        "        of code gets executed:\n",
        "            `variable_1 + variable_2`\n",
        "        then invocation would be\n",
        "            `variable_1.__add__(another=variable_2)` # self=variable_1\n",
        "            \n",
        "        To have the entire \"chain of information processing\" trackable, \n",
        "        we take the computation and management in our own hands:\n",
        "            1. create a *managed* operator to do the addition\n",
        "            2. create a new variable to contain the result -- do NOT\n",
        "              forget to tell the new variable where it comes from:\n",
        "              the operation we just created above.\n",
        "        \"\"\"\n",
        "        add_op = NeuOpAdd(self, another) # create an add-op, using self and \n",
        "            # another as its inputs.\n",
        "        return NeuData(add_op.forward(), from_op=add_op) # create a new \n",
        "            # variable, maintain the computation structure by \n",
        "            # - storing the value \n",
        "            #   - returned by the newly created op, which is \n",
        "            #   - which is commanded to perform its duty --\n",
        "            #   - adding input-1 and input-2; \n",
        "            # - establishing new var's dependency on the `from_op`\n",
        "            \n",
        "    def backward(self, g=1.0):\n",
        "        \"\"\"\n",
        "        This function creates a ring in the chain of back propagation.\n",
        "        When  informed that the change of my value will cause a change\n",
        "        in the final objective `L` by a certain amount `g`, the variable\n",
        "            1. accumulates `g` to `self.grad`, as one variable can have\n",
        "              multiple ways of affecting L, e.g. \n",
        "              L = X1 + Y; \n",
        "              Y = X1 + X2\n",
        "              Then a chenage X1 will affect L in two ways.\n",
        "            2. propagates the information to \n",
        "        \"\"\"\n",
        "        # accumulate grad\n",
        "        self.grad += g\n",
        "        # report\n",
        "        print(f\"{self.name} update grad: {self.grad-g:.3g} \"\n",
        "              f\"-> {self.grad:.3g}\")\n",
        "        # backprop grad\n",
        "        if self.from_op is not None:\n",
        "            self.from_op.backward(g)\n",
        "            \n",
        "            \n",
        "class NeuOpAdd:\n",
        "    op_count = 0 # global operator counter\n",
        "    \n",
        "    def __init__(self, oprand1, oprand2, name=None):\n",
        "        \"\"\"\n",
        "        Init of a PLUS operator, allocate an ID, ensure a name and\n",
        "        link the operator to the oprands.\n",
        "        \"\"\"\n",
        "        self.id_ = f\"Op+{NeuOpAdd.op_count}\"\n",
        "        NeuOpAdd.op_count += 1\n",
        "        self.name = self.id_ if name is None else name\n",
        "        # establish link to oprands\n",
        "        self.oprand1 = oprand1\n",
        "        self.oprand2 = oprand2\n",
        "        # report its creation\n",
        "        print(f\"Operator {self.name} has been created, getting inputs\"\n",
        "              f\" from {self.oprand1.name} and {self.oprand2.name}\")\n",
        "        \n",
        "    def forward(self):\n",
        "        \"\"\"\n",
        "        The computation. It is trivial for this operator.\n",
        "        \"\"\"\n",
        "        resu = self.oprand1.data + self.oprand2.data\n",
        "        # report\n",
        "        print(f\"{self.name} forwarding... \\n\"\n",
        "              f\"\\t input-1 {self.oprand1.name}: value {self.oprand1.data:.3g}\\n\"\n",
        "              f\"\\t input-2 {self.oprand2.name}: value {self.oprand2.data:.3g}\\n\"\n",
        "              f\"\\t ouput: {resu:.3g}\")\n",
        "        return resu      \n",
        "    \n",
        "    def backward(self, g):\n",
        "        \"\"\"\n",
        "        As a simple +, both inputs has equal effect on its output, i.e.\n",
        "        if the output of THIS + OPERATOR affects the global L by a factor \n",
        "        of g, so does the input-1 as well as input-2\n",
        "        \"\"\"\n",
        "        # backprop for op-1\n",
        "        op1_g = g\n",
        "        # report\n",
        "        print(f\"{self.name} transforming grad {g:.3g} to {op1_g:.3g}\"\n",
        "              f\" and pass to {self.oprand1.name}\")\n",
        "        self.oprand1.backward(op1_g)\n",
        "        \n",
        "        # backprop for op-2\n",
        "        op2_g = g\n",
        "        print(f\"{self.name} transforming grad {g:.3g} to {op2_g:.3g}\"\n",
        "              f\" and pass to {self.oprand2.name}\")\n",
        "        self.oprand2.backward(op2_g)\n",
        "        \n",
        "        \n",
        "              "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6U8XJ2UoOwSy",
        "colab_type": "text"
      },
      "source": [
        "__EXERCISE__\n",
        "\n",
        "Run the following code blocks and explain the outputs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yZLZVBUMOwSy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x0 = NeuData(4, name=\"x0\")\n",
        "x1 = NeuData(3, name=\"x1\")\n",
        "L = x0 + x1\n",
        "print(\"================\\nComputation completed. \\n\"\n",
        "      f\"L name:{L.name}, id:{L.id_}, value:{L.data}\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MNTwYR3wOwSz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "L.backward()\n",
        "del x0, x1, L # explicitly remove those variables lest confusion"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UDdqMP_YOwS0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UoK8QI1IOwS1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x0 = NeuData(4, name=\"x0\")\n",
        "x1 = NeuData(3, name=\"x1\")\n",
        "x2 = NeuData(10, name=\"x2\")\n",
        "x3 = x0 + x1\n",
        "L = x3 + (x2 + x3)\n",
        "L.backward(1.0)\n",
        "del x0, x1, x2, x3, L"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vmk-ebdTOwS3",
        "colab_type": "text"
      },
      "source": [
        "### 2.2.2 Full Version of Managed Variable/Op/Func/NeuralNets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CwhgvkQ3OwS4",
        "colab_type": "text"
      },
      "source": [
        "We provide a primary but full-fledged computation framework of constructing NN models that can perform automatic backprop PROGRAMMING."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "code_folding": [
          2,
          36,
          53,
          70,
          83,
          103,
          124
        ],
        "id": "G_iz-McdOwS5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "########################################\n",
        "# Ops\n",
        "VERBOSE = True\n",
        "class NeuOp(object):\n",
        "    \"\"\"\n",
        "    Now we define a system of operators, this is the root class\n",
        "    defining the basic bookkeeping behaviour of operators.\n",
        "    \"\"\"\n",
        "    op_count = 0\n",
        "    \n",
        "    def __init__(self, name=None):\n",
        "        super(NeuOp, self).__init__()\n",
        "        self.id_ = f\"Op{NeuOp.op_count}\"\n",
        "        NeuOp.op_count += 1\n",
        "        if name is None:\n",
        "            name = self.id_\n",
        "        self.name = name\n",
        "    \n",
        "    def backward(self, g):\n",
        "        raise NotImplementedError\n",
        "        \n",
        "    def forward(self):\n",
        "        raise NotImplementedError\n",
        "        \n",
        "        \n",
        "    def make_animation_forward_(self, resu):\n",
        "        try:\n",
        "            g_animator.make_animation_forward(self, resu)\n",
        "        except Exception as e:\n",
        "            pass\n",
        "            \n",
        "    def make_animation_backward_(self, msgs):\n",
        "        try:\n",
        "            g_animator.make_animation_backward(self, msgs)\n",
        "        except Exception as e:\n",
        "            pass\n",
        "        \n",
        "class UnaryNeuOp(NeuOp):\n",
        "    \"\"\"\n",
        "    Operators who has only one oprand (input).\n",
        "    \"\"\"\n",
        "    def __init__(self, oprand, name=None):\n",
        "        super(UnaryNeuOp, self).__init__(name)\n",
        "        self.oprand = oprand\n",
        "        try:\n",
        "            g_animator.make_animation_create(self)\n",
        "        except:\n",
        "            pass\n",
        "        \n",
        "    def backward(self, g):\n",
        "        self.make_animation_backward_([\n",
        "            dict(back_node_id=self.oprand.id_, msg=g)])\n",
        "        self.oprand.backward(g)\n",
        "                \n",
        "class NeuOpNeg(UnaryNeuOp):\n",
        "    \"\"\"\n",
        "    Negation, the \"-\" operator.\n",
        "    \"\"\"\n",
        "    def __init__(self, oprand, name=\"-\"):\n",
        "        super(NeuOpNeg, self).__init__(oprand, name)\n",
        "    \n",
        "    def backward(self, g):\n",
        "        bg = -g\n",
        "        super(NeuOpNeg, self).backward(bg)\n",
        "\n",
        "        \n",
        "    def forward(self):\n",
        "        resu = -self.oprand.data\n",
        "        self.make_animation_forward_(resu)\n",
        "        return resu\n",
        "        \n",
        "class BinaryNeuOp(NeuOp):\n",
        "    \"\"\"\n",
        "    Binary Operators\n",
        "    \"\"\"\n",
        "    def __init__(self, oprand1, oprand2, name=None):\n",
        "        super(BinaryNeuOp, self).__init__(name)\n",
        "        self.oprand1 = oprand1\n",
        "        self.oprand2 = oprand2\n",
        "        try:\n",
        "            g_animator.make_animation_create(self)\n",
        "        except:\n",
        "            pass\n",
        "    \n",
        "class NeuOpAdd(BinaryNeuOp):\n",
        "    \"\"\"\n",
        "    Our old friend \"+\"\n",
        "    \"\"\"\n",
        "    def __init__(self, oprand1, oprand2, name=\"+\"):\n",
        "        super(NeuOpAdd, self).__init__(oprand1, oprand2, name)\n",
        "    \n",
        "    def backward(self, g):\n",
        "        self.make_animation_backward_([\n",
        "            dict(back_node_id=self.oprand1.id_, msg=g),\n",
        "            dict(back_node_id=self.oprand2.id_, msg=g)])\n",
        "        \n",
        "        self.oprand1.backward(g)\n",
        "        self.oprand2.backward(g)\n",
        "        \n",
        "    def forward(self):\n",
        "        resu = self.oprand1.data + self.oprand2.data\n",
        "        self.make_animation_forward_(resu)\n",
        "        return resu\n",
        "    \n",
        "class NeuOpMul(BinaryNeuOp):\n",
        "    \"\"\"* operator, note the backprop\"\"\"\n",
        "    def __init__(self, oprand1, oprand2, name=\"*\"):\n",
        "        super(NeuOpMul, self).__init__(oprand1, oprand2, name)\n",
        "    \n",
        "    def backward(self, g):\n",
        "        # output = in1 * in2, so dL/d_output * in2 = dL/d_in1, similarly for d_in2\n",
        "        self.make_animation_backward_([\n",
        "            dict(back_node_id=self.oprand1.id_, msg=g * self.oprand2.data),\n",
        "            dict(back_node_id=self.oprand2.id_, msg=g * self.oprand1.data)])\n",
        "        \n",
        "        self.oprand1.backward(g * self.oprand2.data)\n",
        "        self.oprand2.backward(g * self.oprand1.data)\n",
        "        \n",
        "    def forward(self):\n",
        "        resu = self.oprand1.data * self.oprand2.data\n",
        "        self.make_animation_forward_(resu)\n",
        "        return resu\n",
        "\n",
        "########################################\n",
        "# Data\n",
        "class NeuData:\n",
        "    d_count = 0\n",
        "    \n",
        "    def __init__(self, data=0, grad=None, from_op=None, name=None):\n",
        "        # essential information\n",
        "        self.data = data\n",
        "        self.grad = grad\n",
        "        self.from_op = from_op\n",
        "        \n",
        "        # comp. graph maintainance and visualisation\n",
        "        self.id_ = f\"D{NeuData.d_count}\"\n",
        "        NeuData.d_count += 1\n",
        "        if name is None:\n",
        "            name = self.id_\n",
        "        self.name = name\n",
        "                \n",
        "        try:\n",
        "            g_animator.make_animation_create(self)\n",
        "        except:\n",
        "            pass\n",
        "        \n",
        "    def __neg__(self):\n",
        "        neg_op = NeuOpNeg(self)\n",
        "        return NeuData(neg_op.forward(), from_op=neg_op)\n",
        "        \n",
        "    def __add__(self, another):\n",
        "        add_op = NeuOpAdd(self, another)\n",
        "        return NeuData(add_op.forward(), from_op=add_op)\n",
        "    \n",
        "    def __sub__(self, another):\n",
        "        add_op = NeuOpAdd(self, -another)\n",
        "        return NeuData(add_op.forward(), from_op=add_op)\n",
        "    \n",
        "    def __mul__(self, another):\n",
        "        mul_op = NeuOpMul(self, another)\n",
        "        return NeuData(mul_op.forward(), from_op=mul_op)\n",
        "    \n",
        "    def backward(self, g=1.0):\n",
        "        if self.grad is None:\n",
        "            self.grad = g\n",
        "        else:\n",
        "            self.grad += g\n",
        "            \n",
        "        if VERBOSE:\n",
        "            print(f\"{self.name} grad: {self.grad:.3f}\")\n",
        "        if self.from_op is not None:\n",
        "            self.from_op.backward(g)\n",
        "            \n",
        "    def __setattr__(self, name, value):\n",
        "        \n",
        "        if name == \"grad\" and value is not None:\n",
        "            try:\n",
        "                g_animator.make_animation_backward(\n",
        "                    self, \n",
        "                    [dict(node_id=self.id_, msg=value)])\n",
        "            except Exception as e:\n",
        "                pass\n",
        "        \n",
        "        super(NeuData, self).__setattr__(name, value)\n",
        "            \n",
        "        \n",
        "    def __str__(self):\n",
        "        return f\"{self.data}\"\n",
        "    \n",
        "    def __repr__(self):\n",
        "        return f\"{self.name}:{self.data:.3f}\" if self.grad is None \\\n",
        "            else f\"{self.name}:{self.data:.3f}:{self.grad:.3f}\"\n",
        "    \n",
        "########################################\n",
        "# Elementwise Functions\n",
        "import math\n",
        "class NeuFunction(object):\n",
        "    \"\"\"\n",
        "    Functions are customised Uniary operators.\n",
        "    The barrier for understanding is the dealing with backprop.\n",
        "    We create a unary op, and hack the object's back-prop function.\n",
        "    \"\"\"\n",
        "    def __init__(self, name=None):\n",
        "        super(NeuFunction, self).__init__()\n",
        "        self.name = name\n",
        "        \n",
        "    def forward(self, x):\n",
        "        import types\n",
        "        op = UnaryNeuOp(x, name=self.name)\n",
        "        # op will be our placeholder in the differentiable computational graph.\n",
        "        # we will never use op's forward computation (not defined anyway)\n",
        "        # but we will take advantage of the *automatic* invocation of op's\n",
        "        # `backward` method, which will be called when the output data pass\n",
        "        # gradient messages through. \n",
        "        \n",
        "\n",
        "        # We hijack the op's function and let it point to the `backward` function\n",
        "        # of THIS object. NOTE, not THIS CLASS, `self` here would represent an instance\n",
        "        # of a SUBCLASS, which will implement appropriate gradient transform.\n",
        "        # See the Sigmoid function example below.\n",
        "        op_original_back = op.backward\n",
        "        def _back(op_instance, g):\n",
        "            g_back = self.backward(op_instance, g) # self is subclass instance\n",
        "            op_original_back(g_back)\n",
        "        op.backward = types.MethodType(_back, op) # wrap it as a class-method function.\n",
        "            # see https://stackoverflow.com/questions/10374527/dynamically-assigning-function-implementation-in-python\n",
        "            # Amber's answer.\n",
        "        return op\n",
        "        \n",
        "    def backward(self, g):\n",
        "        raise NotImplementedError\n",
        "        \n",
        "    def __call__(self, x):\n",
        "        \"\"\"\n",
        "        This will make the object \"callable\", i.e.\n",
        "        f(x) === f.forward(x)\n",
        "        \"\"\"\n",
        "        return self.forward(x)\n",
        "\n",
        "def _sigmoid(x):\n",
        "    return 1 / (1 + math.exp(-x))\n",
        "\n",
        "class FuncSigmoid(NeuFunction):\n",
        "    def __init__(self, name=None):\n",
        "        super(FuncSigmoid, self).__init__(name)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        op = super(FuncSigmoid, self).forward(x)\n",
        "        resu = _sigmoid(x.data)\n",
        "        op.make_animation_forward_(resu)\n",
        "        out = NeuData(resu, from_op=op)\n",
        "        return out\n",
        "    \n",
        "    def backward(self, op, g):\n",
        "        x = op.oprand\n",
        "        y = _sigmoid(x.data)\n",
        "        g = g * y * (1 - y)\n",
        "        if VERBOSE:\n",
        "            print(f\"{self.name} pass-grad: {g:.3f}\")\n",
        "        return g\n",
        "\n",
        "########################################\n",
        "# Neural network components\n",
        "\n",
        "def flatten_data(dlist):\n",
        "    \"\"\"Yield items from any nested iterable; see Reference.\"\"\"\n",
        "    from collections.abc import Iterable\n",
        "    for x in dlist:\n",
        "        if isinstance(x, Iterable):\n",
        "            for sub_x in flatten_data(x):\n",
        "                yield sub_x\n",
        "        else:\n",
        "            assert isinstance(x, NeuData), ValueError(\"Must be neu data\")\n",
        "            yield x\n",
        "            \n",
        "class NeuParameterisedModule(object):\n",
        "    module_count = 0\n",
        "    def __init__(self, name=None):\n",
        "        if name is None:\n",
        "            name = f\"nn{NeuParameterisedModule.module_count}\"\n",
        "        self.name = name\n",
        "        NeuParameterisedModule.module_count += 1\n",
        "        self.parameters_ = []\n",
        "        \n",
        "    def __setattr__(self, name, value):\n",
        "        \"\"\"\n",
        "        We hack the setting attribute to do bookkeeping of learnable parameters:\n",
        "        \"\"\"\n",
        "        if name == \"parameters_\": # parameters are directly set\n",
        "            print(\"set param\")\n",
        "            value = list(flatten_data(value))\n",
        "            \n",
        "        if isinstance(value, NeuParameterisedModule): # member sub-modules added\n",
        "            print(\"add sub module\")\n",
        "            self.parameters_ += value.parameters_\n",
        "            \n",
        "        super(NeuParameterisedModule, self).__setattr__(name, value)\n",
        "    \n",
        "        \n",
        "    def forward(self, x):\n",
        "        raise NotImplementedError\n",
        "        \n",
        "    def __call__(self, x):\n",
        "        return self.forward(x)\n",
        "\n",
        "import random    \n",
        "class Linear(NeuParameterisedModule):\n",
        "    \"\"\"\n",
        "    Linear layer\n",
        "    \"\"\"\n",
        "    def __init__(self, in_features, out_features, name=None):\n",
        "        super(Linear, self).__init__(name)\n",
        "            \n",
        "        self.weights = \\\n",
        "            [[NeuData(random.gauss(0, 1), name=f\"{self.name}:w{_o,_i}\") \n",
        "              for _i in range(in_features)]\n",
        "             for _o in range(out_features)]\n",
        "        self.bias = \\\n",
        "            [NeuData(0, name=f\"{self.name}:b{_o}\") \n",
        "             for _o in range(out_features)]\n",
        "        \n",
        "        self.parameters_ = self.weights + self.bias\n",
        "        \n",
        "    def forward(self, x):\n",
        "        from functools import reduce\n",
        "        out = []\n",
        "        for w, b in zip(self.weights, self.bias):\n",
        "            weighted = [wj * xj for wj, xj in zip(w, x)]\n",
        "            out.append(reduce(lambda a1, a2:a1 + a2, weighted + [b,]))\n",
        "            \n",
        "        return out\n",
        "    \n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kbuRYY7gOwS6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Unit test of Functions\n",
        "f = FuncSigmoid()\n",
        "x1 = NeuData(0.2, name=\"x1\")\n",
        "w1 = NeuData(3, name=\"w1\")\n",
        "h = x1 * (w1 * w1)\n",
        "y = f(h) + f(w1)\n",
        "y.backward()\n",
        "print(f\"Auto-back w1.grad: {w1.grad}\")\n",
        "\n",
        "# Verify numerically\n",
        "f = lambda x: 1 / (1 + math.exp(-x))\n",
        "x1 = 0.2\n",
        "w1 = 3\n",
        "h = x1 * (w1 * w1)\n",
        "y = f(h) + f(w1)\n",
        "\n",
        "t_eps = 1e-3\n",
        "w1 += t_eps\n",
        "h = x1 * (w1 * w1)\n",
        "y_new = f(h) + f(w1)\n",
        "print(f\"Numerical w1.grad: {(y_new - y) / t_eps}\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IQ5DMAxGOwS9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Test using our module to create a neural net\n",
        "class MyNN(NeuParameterisedModule):\n",
        "    def __init__(self, name=\"nn\"):\n",
        "        super(MyNN, self).__init__(name)\n",
        "        self.lin1 = Linear(in_features=2, out_features=3, name=f\"{self.name}:l1\")\n",
        "        self.lin2 = Linear(in_features=3, out_features=1, name=f\"{self.name}:l2\")\n",
        "        \n",
        "    def forward(self, x):\n",
        "        f = FuncSigmoid()\n",
        "        pre_h = self.lin1(x)\n",
        "        h = [ f(hi) for hi in pre_h ]\n",
        "        pre_out = self.lin2(h)\n",
        "        out = [ f(yi) for yi in pre_out ]\n",
        "        return out\n",
        "    \n",
        "random.seed(42)    \n",
        "mynn = MyNN()\n",
        "print(mynn.parameters_)\n",
        "y = mynn([NeuData(1), NeuData(2)])\n",
        "print(y[0])\n",
        "y[0].backward()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DhA0aeIrVnTi",
        "colab_type": "text"
      },
      "source": [
        "## 2.3 Deep Architecture Applied"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5RlJvHgkZ2V-",
        "colab_type": "text"
      },
      "source": [
        "### 2.3.1 Prepare Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "srt1kTOiIjW8",
        "colab_type": "text"
      },
      "source": [
        "We copy the library, data and environment preparation below for quick and show reference."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hXV1VixXV0a6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The example is adopted from pytorch document:\n",
        "# see https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html#sphx-glr-beginner-blitz-cifar10-tutorial-py\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as fn\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Change the type of the virtual machine in \"Runtime/Runtime Type\"\n",
        "device = torch.device(\"cuda:0\") if torch.cuda.is_available() \\\n",
        "    else torch.device(\"cpu\")\n",
        "\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=4,\n",
        "                                         shuffle=False, num_workers=2)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "\n",
        "\n",
        "# functions to show an image\n",
        "def imshow(img):\n",
        "    img = img / 2 + 0.5     # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# get some random training images\n",
        "dataiter = iter(trainloader)\n",
        "images, labels = dataiter.next()\n",
        "\n",
        "# show images\n",
        "imshow(torchvision.utils.make_grid(images))\n",
        "\n",
        "# print labels\n",
        "print(' '.join(classes[l] for l in labels))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WuGE_KsBZ66U",
        "colab_type": "text"
      },
      "source": [
        "### 2.3.2 Define and Train Network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AZaYE4LnXG8A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size = x.shape[0]\n",
        "        h = fn.max_pool2d(fn.relu(self.conv1(x)), kernel_size=2)\n",
        "        h = fn.max_pool2d(fn.relu(self.conv2(h)), kernel_size=2)\n",
        "        h = h.view(batch_size, -1)\n",
        "        h = fn.relu(self.fc1(h))\n",
        "        h = fn.relu(self.fc2(h))\n",
        "        y = self.fc3(h)\n",
        "        return y\n",
        "\n",
        "net = Net()\n",
        "net = net.to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VrSaT3mWXWji",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.optim as optim\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "for epoch in range(2):  # loop over the dataset multiple times\n",
        "\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        # get the inputs; data is a list of [inputs, labels]\n",
        "        inputs, labels = data\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        outputs = net(inputs)\n",
        "        loss = fn.cross_entropy(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # print statistics\n",
        "        running_loss += loss.item()\n",
        "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
        "            print('[%d, %5d] loss: %.3f' %\n",
        "                  (epoch + 1, i + 1, running_loss / 2000))\n",
        "            running_loss = 0.0\n",
        "\n",
        "print('Finished Training')\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}