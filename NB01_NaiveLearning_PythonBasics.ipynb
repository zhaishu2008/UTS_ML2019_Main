{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# 1 Model of Learning Procedure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Naive Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We implement the naive learning scheme. More specifically we want to represent a map from of $X$ to $y$ intuitively -- by using a complete table of all possibilities exhaustively. To make it possible, we limit $X$ to be a discrete 2D tuple -- be one of a dot in a 2D square array, you will see examples shortly -- and $y$ to be 0 or 1. \n",
    "\n",
    "- Build a `Python object` to represent all the possible relationship between $X$ and $y$\n",
    "- Given a training sample, i.e. a pair of $X$ and $y$, the learning-model object can eliminate all the possibilities that are incompatible with the observation.\n",
    "- Given a test sample, i.e. an $X$ without $y$, the learning-model object can return all the possibilities and their respective $y$-values at the test $X$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Represent All X-Samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Attempt Round 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def generate_all_X_space_samples():\n",
    "    \"\"\"\n",
    "    As the function name shows,  here we want to return the \n",
    "    complete set of possible X values. The straightforward \n",
    "    implementation of the X-space is a list of tuples. Let us \n",
    "    consider a simple range: the integers from 0 to N-1, and \n",
    "    use this range for both dimensions. Say N=3, we want to \n",
    "    generate X-samples as\n",
    "    [\n",
    "        (0, 0),\n",
    "        (0, 1),\n",
    "        (0, 2),\n",
    "        (1, 0),\n",
    "        (1, 1),\n",
    "        (1, 2),\n",
    "        (2, 0),\n",
    "        (2, 1),\n",
    "        (2, 2),\n",
    "    ]\n",
    "    \n",
    "    For small N, we can explicitly write out the list, but we need \n",
    "    a program to generate such a list for arbitrary N:\n",
    "    \"\"\"\n",
    "    \n",
    "    # Let's make an empty list\n",
    "    X_space = []\n",
    "    \n",
    "    # Study the elements in the example list, and fill up our\n",
    "    # X_space, e.g. by\n",
    "    X_space.append((0, 0)) # A sample in X is a tuple, so we use \n",
    "    # a pair of parentheses, i.e. the input to the \"append\" function\n",
    "    # is \"(0, 0)\", not \"0, 0\", which will be interpreted as 2 inputs.\n",
    "    X_space.append((0, 1))\n",
    "    X_space.append((0, 2))\n",
    "    # ... you can complete the rest if you wish, but better read on.\n",
    "    # we will use smarter methods.\n",
    "    \n",
    "    # Last but note least, \n",
    "    return X_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<span style=\"color:#006000\"><b>EXERCISE</b></span>\n",
    "In the cell below, experiment with the function `generate_all_X_space_samples` we just defined. You can manipulate the definition of the function  and  observe the change of its behaviour. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X_space = generate_all_X_space_samples()\n",
    "print(X_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Attempt Round 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def generate_all_X_space_samples():\n",
    "    \"\"\"\n",
    "    We will use loops to generate the tuples!\n",
    "    \"\"\"\n",
    "    \n",
    "    # Let's make an empty list\n",
    "    X_space = []\n",
    "    \n",
    "    # Simple observation shows the first 3 tuples are (0, j)\n",
    "    # and j is running from 0 to 3 (exclusive, Python convention)\n",
    "    \n",
    "    # This is the perfect case to use a for-loop, so we can write the\n",
    "    # list building program this way:\n",
    "    \n",
    "    # for j in range(3):\n",
    "    #     X_space.append((0, j))\n",
    "    # for j in range(3):\n",
    "    #     X_space.append((1, j))\n",
    "    # for j in range(3):\n",
    "    #     X_space.append((2, j))\n",
    "    \n",
    "    # You may have noticed, the first element in each tuple in those\n",
    "    # loops runs from 0 to 3 (exclusive) as well, and can also be\n",
    "    # managed by a loop\n",
    "    for i in range(3):\n",
    "        for j in range(3):\n",
    "            X_space.append((i, j))\n",
    "    return X_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<span style=\"color:#006000\"><b>EXERCISE</b></span>\n",
    "Experiment with the for loop above. Try to generate x spaces of different sizes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Attempt Round 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We further adjust our implementation in two ways:\n",
    "\n",
    "1. It is natural for the function to be flexible so we can generate different sizes of X conveniently without rewriting the code every time.\n",
    "\n",
    "2. Python provides a more natural way to write loops to generate object collections (e.g. list of objects). \n",
    "\n",
    "Let's try 2 in the cell below and then re-write our X-sample generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# 1. building list by appending one element each time\n",
    "my_list_a = []\n",
    "for i in range(5):\n",
    "    my_list_a.append(i**2) # square\n",
    "print(\"List-a of Sqr for [0, 5):\", my_list_a)\n",
    "\n",
    "# 2. Write the message above naturally as python code\n",
    "my_list_b = [i**2 for i in range(5)] # Bracket [..] to construct a list\n",
    "print(\"List-b of Sqr for [0, 5):\", my_list_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<span style=\"color:#006000\"><b>EXERCISE</b></span>\n",
    "Try to generate a list of even numbers from 2 to 10 (exclusive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# 3. Powerful generator\n",
    "# The element object can be complex object. \n",
    "# The []-generating loop can be nested.\n",
    "# The generation process can be conditioned, too.\n",
    "\n",
    "my_list_c = [(j, j + i**2) for i in range(10)\n",
    "             if i % 2 == 0\n",
    "             for j in range(100, 600, 100)\n",
    "             if j != 300]\n",
    "print(my_list_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "__CAVEAT__: Although looking very neat, internally this kind of generator does not save you any time or space complexity.  It is purely for readability,  so use it only to IMPROVE the readability!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def generate_all_X_space_samples(N):\n",
    "    \"\"\"\n",
    "    Generate complete sample of X-space\n",
    "    :param N: Discrete X-space dimension size. The size is homogeneous\n",
    "      in all dimensions.\n",
    "    :type N: int\n",
    "    \"\"\"\n",
    "    \n",
    "    return [(i, j) for i in range(N)\n",
    "            for j in range(N)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<span style=\"color:#006000\"><b>EXERCISE</b></span>\n",
    "In the cell below, experiment with the new function `generate_all_X_space_samples` we just defined. Please try different X-space sizes and investigate different X-samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X = generate_all_X_space_samples(3)\n",
    "print(\"There are {} samples in X-space.\".format(len(X))) # {}-format\n",
    "# is used to inject some information from variables to a string.\n",
    "print(\"All samples:\\n\\t\", X) # \\n: new line, \\t indent\n",
    "\n",
    "# You can also investigate using multiple print's\n",
    "for sample_id in range(len(X)): # Try to figure out the construction\n",
    "    print(\"Sampe {}: {}\".format(sample_id, X[sample_id]))\n",
    "    \n",
    "# You can use [:] indexing to conveniently check a subset of data samples\n",
    "print(\"Sample 1-5 (exc):\", X[1:5])\n",
    "# [:End] means start from 0\n",
    "print(\"Sample 0-3 (exc):\", X[:3])\n",
    "# Similarly, [Start:] means until the end\n",
    "print(\"Sample 3-Last (inc):\", X[3:])\n",
    "# You can use -i (<0 index) to represent \"reversing from the end\"\n",
    "print(\"Last Sample:\", X[-1])\n",
    "print(\"Sample 3-Last (exc):\", X[3:-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Attempt Round 4 -- Using Numpy Arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Python list is convenient for us to store and access data samples.  When it comes to doing analysis or machine learning algorithms it is more convenient if we can easily access individual attributes or perform computational operations on specified parts of the data. We will use numpy library, it is designed to manage array data. Numpy arrays can also be easily converted to/from `data frames`, `GPU device arrays`, `images (pixel arrays)`, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Let us use the numpy library\n",
    "import numpy as np # the \"as\" is optional and to save typing\n",
    "\n",
    "def generate_all_X_space_samples_np(N):\n",
    "    \"\"\"\n",
    "    :param N: X-space will be an N by N discrete-valued array\n",
    "    \"\"\"\n",
    "    \n",
    "    # Let's make an empty list\n",
    "    X_space = np.zeros((N**2, 2)) # the \n",
    "    \n",
    "    # Loop is similar to that in Round2\n",
    "    # except that all samples are created at the\n",
    "    # beginning, and we now use an index to loop over them\n",
    "    index = 0\n",
    "    for i in range(N):\n",
    "        for j in range(N):\n",
    "            X_space[index][0] = i\n",
    "            X_space[index][1] = j\n",
    "            index += 1\n",
    "    return X_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X_np = generate_all_X_space_samples_np(3)\n",
    "print(X_np)\n",
    "print(type(X_np)) # Note the type is a np-array\n",
    "# Check out a sample\n",
    "i = 3\n",
    "print(\"An X-Sample[{}]:{}\".format(i, X_np[i]))\n",
    "# Check attribute-0 for all samples\n",
    "j = 0\n",
    "print(\"X-Attribute[{}]:{}\".format(j, X_np[:, j]))\n",
    "# [:, 0]: take from all (:) samples, the attribute-0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<span style=\"color:#006000\"><b>EXERCISE</b></span>\n",
    "Please check (print out) the second (index=1) attribute for samples 1-5 (exclusive). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Numpy arrays provide interface to apply computations for all elements. E.g. we may want to scale all elements in $X$ between $[0, 1]$. Numpy arrays provide interface to apply computations for all elements. Using an ordinary Python list,  we need to reconstruct another list to store the result,  and perform the competition element by element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def scale_X_to_0_1(X, N):\n",
    "    \"\"\"\n",
    "    Get a new list scaling the elements in X by 1/N.\n",
    "    \"\"\"\n",
    "    new_list = []\n",
    "    for x in X: # you can iterate over each element (a tuple in x)\n",
    "        # now x is one data sample in X, such as (0, 2)\n",
    "        new_list.append((x[0]/N, x[1]/N))\n",
    "    return new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X = generate_all_X_space_samples(5)\n",
    "X1 = scale_X_to_0_1(X, 5)\n",
    "print(X1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# On te other hand, operating on numpy array is much easier\n",
    "X_np = generate_all_X_space_samples_np(5)\n",
    "X1_np = X_np/5\n",
    "print(X1_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Not only the code is more concise. The computation is done internally using fast C implementation, and therefore more efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%timeit X1 = scale_X_to_0_1(X, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%timeit X1 = X1_np = X_np/5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<span style=\"color:#006000\"><b>EXERCISE</b></span>\n",
    "Note the time units $\\mu$s ($10^{-6}$ sec) / ns ($10^{-9}$ sec) used in the measurement above. You can make a larger matrix e.g. using `generate_all_X_space_samples(500)` and compare the difference. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Finally, `numpy` provides an interface to generate this kind of X samples,  by sampling a grid in a multidimensional space. `meshgrid` takes the grid positions at each dimension and returns the grid matrices. In our example, matrix-0 for attribute-1, and matrix-1 for attribute-0 (the order of attributes can be adjusted when we composing the final X, and is not essential). I will not go to details please find more about the function referring to the [doc](https://docs.scipy.org/doc/numpy/reference/generated/numpy.meshgrid.html).\n",
    "\n",
    "Please study the following example for some basic array operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def generate_all_X_space_samples_np(N):\n",
    "    \"\"\"\n",
    "    :param N: X-space will be an N by N discrete-valued array\n",
    "    \"\"\"\n",
    "    X0, X1 = np.meshgrid(np.arange(N), np.arange(N))\n",
    "    # We will have the following for N=3\n",
    "    # X0:      X1:\n",
    "    # 0 1 2    0 0 0\n",
    "    # 0 1 2    1 1 1\n",
    "    # 0 1 2    2 2 2\n",
    "    \n",
    "    # X0, if \"flattened\", becomes\n",
    "    # 0 1 2 0 1 2 0 1 2\n",
    "    \n",
    "    # flattened X0 and X1 if \"stacked\" becomes\n",
    "    # [[0 1 2 0 1 2 0 1 2\n",
    "    #  [0 0 0 1 1 1 2 2 2]]\n",
    "    \n",
    "    # The following matrix, \n",
    "    # [[a b c]\n",
    "    #  [d e f]]\n",
    "    # if \"transposed\" (numpy operator \"T\"), becomes\n",
    "    # [[a d]\n",
    "    #  [b e]\n",
    "    #  [c f]]\n",
    "    return np.stack([X0.flatten(), X1.flatten()]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(generate_all_X_space_samples_np(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%timeit generate_all_X_space_samples_np(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%timeit generate_all_X_space_samples(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Finally, we can make version that includes the normalisation \n",
    "# (1/N) in the construction\n",
    "def generate_all_X_space_normalised_samples_np(N):\n",
    "    \"\"\"\n",
    "    :param N: X-space will be an N by N discrete-valued array\n",
    "    \"\"\"\n",
    "    X0, X1 = np.meshgrid(np.arange(N), np.arange(N))\n",
    "    return np.stack([X0.flatten(), X1.flatten()]).T / N"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Represent all possible X-y relations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We will create a template from which we can generate objects, which represent  _generic_ relationship from all $X$-samples in to binary $y$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Initialise the framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Let us first prepare the X-space as discrete samples as above. \n",
    "# And before we start building all the possible X-y mappings. \n",
    "# It is sensible to have an idea about how many such \n",
    "# mappings we are going to consider.\n",
    "\n",
    "# So here is our first attempt of making the object template \n",
    "# of the all-inclusive mapping representation.\n",
    "class CompleteDiscrete2DBinaryMapping(object):\n",
    "    \"\"\"\n",
    "    An exhaustive representation of 2D X space to binary targets.\n",
    "    The 2D space is represented using discrete grid points.\n",
    "    \"\"\"\n",
    "    def __init__(self, N):\n",
    "        \"\"\"\n",
    "        Create an object representing all possible mappings from \n",
    "        2D grid points to {0, 1}. \n",
    "        :param N: X-space samples are N by N grid in [0, 1)**2\n",
    "        \"\"\"\n",
    "        self.grid_x = generate_all_X_space_samples_np(N)\n",
    "        self.h_size = 2 ** (N**2)\n",
    "        \n",
    "    def size(self):\n",
    "        \"\"\"\n",
    "        Total number of possible mappings.\n",
    "        \n",
    "        Note this tend to be really large number for any\n",
    "        respectable N.\n",
    "        \"\"\"\n",
    "        return self.h_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "complete_model = CompleteDiscrete2DBinaryMapping(10)\n",
    "print(\"We are going to build {} different mappings.\"\n",
    "       .format(complete_model.size()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<span style=\"color:#006000\"><b>EXERCISE</b></span>\n",
    "Please review our discussion in class and figure out why we compute the size of the possible mappings to be $2^{N^2}$? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "So for any respectable problem size,  exclusively consider all possibilities is exceeding the capability of a computer. Can we possibly implement such an object?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Focus on prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Yes and no, we employer implement a representing all possible mappings.  But we cannot wait for it to make any useful predictions,  because it takes very long time to work. The point here we will adopt the _duck-typing_ / interface oriented programming protocol to have some object that works. This way of building programs is widely used in Python (and particularly useful in data science where the storage demand can be very large).\n",
    "\n",
    "Duck-typing: \n",
    "\n",
    "> If something that walks like a duck and the quacks like a duck then it is probably a duck.\n",
    "\n",
    "That is, we focused on how the object will be used and maintaining necessary information in working conditions only. \n",
    "\n",
    "Since we are implementing a data model _family_. At any particular call, we need only to specify the $y$ value (0 or 1) for some $X = (X_1 \\in [0, 1), X_2 \\in [0, 1))$ according to a _particular member_ in this family. That is, we do not need to worry about storing all possible mappings at one time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# To be specific, we just need to implement such a function\n",
    "def predict_according_to_hypothesis(X, hypothesis_id):\n",
    "    \"\"\"\n",
    "    :param X: a data has 2 attributes\n",
    "    :param hypothesis_id: a number in 0..1267650600228229401496703205376 \n",
    "        (e.g. N=10)\n",
    "    NOTE: for stand-alone function (not belonging to any class,\n",
    "        \"unbounded\" is the technical term), we don't have the \"self\"\n",
    "        in the first place in the input argument list.    \n",
    "    \"\"\"\n",
    "    y = 0 # or 1\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Predict using one assigned hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Now we need to solve two problems,\n",
    "1. We need to verify the input X as one of the 2D grid points  according to our problem setting.  If it is not, quantise it to one of them.\n",
    "2. Figure out according to the particular mapping specified by `hypothesis_id` (The technical term of such a hypothetical mapping is a _hypothesis_),  what is the corresponding y value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The first problem can be solved by finding the nearest the neighbour to the input X from all the 2D grid points. This, of course, will remind us the nearest neighbour classifier. There is one essential difference though: there is no training data for our nearest neighbour classifier to refer to, so we have to assign some hypotheses, which leads us to the second problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# To compute the nearest neighbour, \n",
    "# please experiment with the following code.\n",
    "X_np = generate_all_X_space_samples_np(2)\n",
    "print(\"2D points\")\n",
    "print(X_np)\n",
    "Xin = np.array((1, 2))\n",
    "print(\"Input X\")\n",
    "print(Xin)\n",
    "print(\"Difference\")\n",
    "print(X_np - Xin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Amazingly, we have implemented the difference between the input $X$ to __each one of the grid points using just one operation__.  This seemingly incompatible substraction has been implemented in numpy using the mechanism _broadcasting_. It allows binary operators to work between one array $A$ of \n",
    "$n_1 \\times n_2 $ and the other $B$ of $n_2$, while considering the larger $A$ to contain $n_1$ small arrays and applying the operation between each of the $n_1$ small arrays and $B$. \n",
    "\n",
    "It also generalises to the case when A is of $n_1 \\times n_2 \\times n_3 \\times n_4$ and B is of $n_3 \\times n_4$. Then we view $A$ as $n_1\\times n_2$ cells and each cell is an $n_3 \\times n_4$ array.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# To compute the nearest neighbour\n",
    "diff = X_np - Xin\n",
    "diff_square = diff ** 2 # each element\n",
    "diff_norm2 = diff_square.sum(axis=1) # summing up every row, so now we have \n",
    "# N**2 distances (same number of X-rows) and need only to find the \n",
    "# smallest one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(\"The index of the nearest x-grid point is {}\"\n",
    "      .format(np.argmin(diff_norm2))) # argmin returns the index of the \n",
    "# smallest element in an array (take care and read doc for multi-dim arrays)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "There we consider what the hypothesis would say about the y value at that particular x-grid point, such as point-3. You may have already guessed as we have totally $N^2$ x-grid points,  and the total number of possible hypotheses is $2^{N^2}$. We are exploring all possible binary combinations with $N^2$ bits. Say, $N=3, N^2=9$, we just count 9-bit binary numbers. And if you ask: what is hypothsis-178’s prediction on the 3rd x-grid point. We can just check the 3rd bit of the binary number corresponding to 178."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# to convert a number to binary format\n",
    "print(\"{:b}\".format(35))\n",
    "# to specify the number of bits\n",
    "print(\"{:9b}\".format(35))\n",
    "# to specify the number of bits and fill unused bits with 0\n",
    "print(\"{:09b}\".format(35))\n",
    "# to specify the number of bits and fill unused bits with 0\n",
    "# and finally take out the 3rd bit\n",
    "print(\"{:09b}\".format(35) [2])\n",
    "\n",
    "# given N, build the \"formatting\" string (a meta string you use to \n",
    "# format other strings\n",
    "N=3\n",
    "print(\"{:0\" + str(N**2) + \"b}\") # \"+\" concatenates strings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Put everything together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class CompleteDiscrete2DBinaryMapping(object):\n",
    "    \"\"\"\n",
    "    An exhaustive representation of 2D X space to binary targets.\n",
    "    The 2D space is represented using discrete grid points.\n",
    "    \"\"\"\n",
    "    def __init__(self, N):\n",
    "        \"\"\"\n",
    "        Create an object representing all possible mappings from \n",
    "        2D grid points to {0, 1}. \n",
    "        :param N: X-space samples are N by N grid in [0, 1)**2\n",
    "        \"\"\"\n",
    "        self.grid_x = generate_all_X_space_samples_np(N)\n",
    "        self.dof = N ** 2 # the degrees of freedom is eaqual to the number\n",
    "        # of grid points at which you can freely choose {0/1} as the \n",
    "        # target value. DoF reduces as you start observing data (when you\n",
    "        # observe the target value at a point, you lose the freedom of\n",
    "        # setting it to arbitrary values)\n",
    "        self.h_size = 2 ** self.dof\n",
    "        \n",
    "    def size(self):\n",
    "        \"\"\"\n",
    "        Total number of possible mappings.\n",
    "        \n",
    "        Note this tend to be really large number for any\n",
    "        respectable N.\n",
    "        \"\"\"\n",
    "        return self.h_size\n",
    "    \n",
    "    def predict_according_to_hypothesis(self, X, hypothesis_id):\n",
    "        \"\"\"\n",
    "        Note, when implement as class method, don't miss \"self\"\n",
    "        :param X: a data has 2 attributes\n",
    "        :param hypothesis_id: a number in 0..1267650600228229401496703205376 \n",
    "            (e.g. N=10)\n",
    "        \"\"\"\n",
    "        X = np.array(X) # make the input format more flexible, e.g.\n",
    "        # you can use [0, 2] (Python list), or (0, 1) (Python tuple)\n",
    "        d = ((self.grid_x - X)**2).sum(axis=1)\n",
    "        bit_id = np.argmin(d)\n",
    "        format_string = \"{:0\" + str(self.dof) + \"b}\"\n",
    "        y = int(format_string.format(hypothesis_id) [bit_id])\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\n",
    "complete_model = CompleteDiscrete2DBinaryMapping(10)\n",
    "print(\"We are going to build {} different mappings.\"\n",
    "       .format(complete_model.size()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# This won't stop!\n",
    "for hypothesis_id in range(complete_model.size()):\n",
    "    print(complete_model\n",
    "          .predict_according_to_hypothesis((8, 7), hypothesis_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Fit to Training Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "(We will start moving faster from here.) Now suppose we are given training samples in the following format: $\\{x_1 = \\langle(0, 1), 1\\rangle, x_2 = \\langle(3, 4), 0\\rangle\\}$. How would the information affect our belief about the $X$-$y$ mapping?\n",
    "\n",
    "We will introduce a method `fit`, which checks consistency between every hypothesis and the observed data and removes those hypotheses that disagree with the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class CompleteDiscrete2DBinaryMapping(object):\n",
    "    def __init__(self, N):\n",
    "        self.grid_x = generate_all_X_space_samples_np(N)\n",
    "        self.dof = N ** 2\n",
    "        self.h_size = 2 ** self.dof\n",
    "        self.inconsistent_hypotheses = []\n",
    "        \n",
    "    def size(self):\n",
    "        return self.h_size\n",
    "    \n",
    "    def predict_according_to_hypothesis(self, X, hypothesis_id):\n",
    "        X = np.array(X)\n",
    "        d = ((self.grid_x - X)**2).sum(axis=1)\n",
    "        bit_id = np.argmin(d)\n",
    "        format_string = \"{:0\" + str(self.dof) + \"b}\"\n",
    "        y = int(format_string.format(hypothesis_id) [bit_id])\n",
    "        return y\n",
    "    \n",
    "    # Let add a `fit` method\n",
    "    def fit(self, X, Y): \n",
    "        \"\"\"\n",
    "        :param X: [M x 2] training data\n",
    "        :param Y: [M] labels\n",
    "        \"\"\"\n",
    "        # Let's check consistency for each training data and each hypothesis \n",
    "        for hid in range(self.h_size):\n",
    "            for x_, y_ in zip(X, Y): \n",
    "                # be careful if the training set contains only 1 sample!\n",
    "                # zip is literally zipping two \"iterables\" so the zipped object\n",
    "                # yield multiple elements in each iteration.\n",
    "                pred = self.predict_according_to_hypothesis(x_, hid)\n",
    "                if pred != y_:\n",
    "                    if hid not in self.inconsistent_hypotheses:\n",
    "                        self.inconsistent_hypotheses.append(hid)\n",
    "                    break # we have determined this hid is bad and no need\n",
    "                    # to continue\n",
    "        \n",
    "        \n",
    "    def predict_trained(self, X):\n",
    "        return [\n",
    "            self.predict_according_to_hypothesis(X, hid)\n",
    "            for hid in range(self.h_size)\n",
    "            if hid not in self.inconsistent_hypotheses\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Test using the example we have seen in class\n",
    "complete_model = CompleteDiscrete2DBinaryMapping(3)\n",
    "X_trn = [\n",
    "    (0, 2),\n",
    "    (1, 2),\n",
    "    (1, 0),\n",
    "    (1, 1),\n",
    "    (2, 0),\n",
    "    (2, 1),\n",
    "]\n",
    "Y_trn = [0, 0, 1, 1, 1, 1]\n",
    "complete_model.fit(X_trn, Y_trn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "complete_model.predict_according_to_hypothesis((0,2), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Let us use the model to predict\n",
    "complete_model.predict_trained((0, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<span style=\"color:#006000\"><b>EXERCISE</b></span>\n",
    "Interpret how `predict_trained` works. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "__Improvement Idea 1__\n",
    "\n",
    "Let's apply the \"duck-typing\" principle again -- we don't need to explicitly find out all inconsistent hypotheses and exclude them when testing. We can construct hypothesis set that is consistent. \n",
    "\n",
    "\n",
    "__Improvement Idea 2__\n",
    "\n",
    "Try to increase the quantisation number $N$ to $4$ (or $5$ if you are in a more adventurous mood) and see how the model works. Next we will introduce limitations on the possibile hypotheses. See below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Data Models (Preview)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<span style=\"color:#006000\"><b>PREVIEW EXERCISE</b></span>\n",
    "Figure out how the \"linear\" model below works. Try to introduce a non-trivial threshold when the hypotheses making predictions (See `PREDICTION BY INDIVIDUAL HYPOTHESIS`). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# A Simple Linear Model Family\n",
    "import numpy as np\n",
    "class LinearHypothesisSpace:\n",
    "    def __init__(self, quant_num=3):\n",
    "        self.quant_num = quant_num\n",
    "        grid_x0, grid_x1 = np.meshgrid(np.arange(self.quant_num),\n",
    "                                       np.arange(self.quant_num))\n",
    "        grid_x0 = grid_x0.flatten()\n",
    "        grid_x1 = grid_x1.flatten()\n",
    "        self.grid_x = np.stack([grid_x0, grid_x1]).T\n",
    "\n",
    "        eps_angle = np.pi / 18\n",
    "        angles = np.arange(0, np.pi, eps_angle)\n",
    "        self.hypotheses = np.zeros((2 * len(angles), self.quant_num ** 2),\n",
    "                                   dtype=np.int)\n",
    "        x0 = grid_x0 - (quant_num - 1) / 2\n",
    "        x1 = grid_x1 - (quant_num - 1) / 2\n",
    "        for i, th in enumerate(angles):\n",
    "            w = min(np.tan(th), 9999)\n",
    "            # ** PREDICTION BY INDIVIDUAL HYPOTHESIS **\n",
    "            ya = (x0 * w - x1 > 0).astype(np.int)\n",
    "            yb = 1 - ya\n",
    "            self.hypotheses[2 * i, :] = ya\n",
    "            self.hypotheses[2 * i + 1, :] = yb\n",
    "        self.sele_hypothesis_id = None\n",
    "\n",
    "    def fit(self, x, y):\n",
    "        x_ind = x[:, 0] + self.quant_num * x[:, 1]\n",
    "        pred_trn = self.hypotheses[:, x_ind]\n",
    "        accu_trn = pred_trn == y[np.newaxis, :]  # type: np.ndarray\n",
    "        accu_trn_n = accu_trn.astype(np.float).sum(axis=1)\n",
    "        self.sele_hypothesis_id = np.argmax(accu_trn_n)\n",
    "\n",
    "    def predict_all_X(self, hypothesis_id=-1):\n",
    "        h = self.hypotheses[self.sele_hypothesis_id] \\\n",
    "            if hypothesis_id == -1 \\\n",
    "            else self.hypotheses[hypothesis_id]\n",
    "        return self.grid_x, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "linear_model0 = LinearHypothesisSpace(3)\n",
    "X_trn = np.array([\n",
    "    (0, 2),\n",
    "    (1, 2),\n",
    "    (1, 0),\n",
    "    (1, 1),\n",
    "    (2, 0),\n",
    "    (2, 1),\n",
    "])\n",
    "Y_trn = np.array([0, 0, 1, 1, 1, 1])\n",
    "linear_model0.fit(X_trn, Y_trn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X_all, y_all = linear_model0.predict_all_X()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Visualing the model behaviour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Finally, let us visualise the model behaviour, we will us an interactive \n",
    "# visualision tool.\n",
    "\n",
    "# NOTE drawing graphs is one noticeable difference between running your\n",
    "# Python notebook on cloud (where the computers don't have screens and have\n",
    "# to deliver graphics objects to your browser to render on YOUR screen), and \n",
    "# on local computer (where graphics display natively using graph interface \n",
    "# provided by your local OS). So we make a bit configuration here. \n",
    "#\n",
    "# If the graphs don't work on your computer, try on colab, or you can \n",
    "# change to classical matplotlib library, which is easier to make working.\n",
    "\n",
    "\n",
    "I_AM_RUNNING_THIS_NOTEBOOK_ON_MY_OWN_COMPUTER = True\n",
    "COLAB = not I_AM_RUNNING_THIS_NOTEBOOK_ON_MY_OWN_COMPUTER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "if COLAB: # We need to upgrade plotly to 4.0 for it to work with colab\n",
    "    # [as of July 2019] this will obsolete soon when Google upgrades colab\n",
    "    !pip install plotly --upgrade\n",
    "    # Peform the same on your own computer if encountering issues, but only\n",
    "    # do it once and for all. colab is a virtual machine, so you need to\n",
    "    # perform the upgrading each time restarting a session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "fig = go.Figure(\n",
    "    data=[go.Scatter(\n",
    "        x=X_all[:, 0], \n",
    "        y=X_all[:, 1], \n",
    "        marker_color=y_all,\n",
    "        marker_size=12,\n",
    "        marker_line_width=2,\n",
    "        mode=\"markers\")],\n",
    "    layout_title_text=\"Prediction on a Discretised 2D X-Space\"\n",
    ")\n",
    "if COLAB:\n",
    "    fig.show(renderer=\"colab\")\n",
    "else:\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Now we can handle decently sized (2D discrete) data space\n",
    "hypothesis_id = 21 # we havn't trained the model, so need to specify which hypo\n",
    "# we want to check\n",
    "linear_model1 = LinearHypothesisSpace(50)\n",
    "X_all, y_all = linear_model1.predict_all_X(hypothesis_id)\n",
    "fig = go.Figure(\n",
    "    data=[go.Scatter(\n",
    "        x=X_all[:, 0], \n",
    "        y=X_all[:, 1], \n",
    "        marker_color=y_all,\n",
    "        marker_size=12,\n",
    "        marker_line_width=2,\n",
    "        mode=\"markers\")],\n",
    "    layout_title_text=\"Prediction on a Discretised 2D X-Space\"\n",
    ")\n",
    "if COLAB:\n",
    "    fig.show(renderer=\"colab\")\n",
    "else:\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Summarise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- We have built a omnipotently useless 2D classifier!\n",
    "- We tried out a linear modeller.\n",
    "- We have learned some useful Python and numpy skills.\n",
    "- We have made nice pictures!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# 2 Model Complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Environment and constant preparation\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "try:\n",
    "    import plotly.graph_objects as go\n",
    "except:\n",
    "    !pip install plotly==4.1.0\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "COLAB = False\n",
    "IRIS_EXPERIMENT_X_SPACE_QUANTISATION_BIN_NUM = 50 # self-interpretation\n",
    "\n",
    "def generate_all_X_space_normalised_samples_np(N):\n",
    "    \"\"\"\n",
    "    :param N: X-space will be an N by N discrete-valued array\n",
    "    \"\"\"\n",
    "    X0, X1 = np.meshgrid(np.arange(N), np.arange(N))\n",
    "    return np.stack([X0.flatten(), X1.flatten()]).T / N"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We will test models of different complexities on the simplified Iris data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Prepare Iris Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let us first prepare the Iris Data into the simplified format. The simplification steps are\n",
    "1. we consider the problem of detecting Versicolour (as the postitive class, class-1), to make it even simpler, I will consider Setosa as the negative class (class-0)\n",
    "2. we use only the first two attributes\n",
    "3. we will discretise the attributes into 50 \"ticks\" -- This is not necessary for data modelling. This is to be consistent with our experiments on \"how data modelling worked in a grid in the entire X-space\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# load the data\n",
    "iris_db = load_iris()\n",
    "all_x = generate_all_X_space_normalised_samples_np(\n",
    "    IRIS_EXPERIMENT_X_SPACE_QUANTISATION_BIN_NUM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "__SHORT-CUT__ The exercises from 2.1.1 to 2.1.4 are for programming skills only. Jump to [2.1.5](#Data-preprocessing-summary) to quickly get preprocessed data, if you want to skip learning programming skills."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Exploring the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<span style=\"color:#006000\"><b>EXERCISE</b></span>\n",
    "Follow the following few code cells, experiment with inspecting the dataset. (Adjust the code, observe what you get and try to explain why)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# BEGIN Data Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# load_iris returns an object `iris_db`, but for now, we don't have much \n",
    "# information on the object. We usually start studying unknown objects\n",
    "# in one of the following two steps.\n",
    "\n",
    "# 1. check what is its `type`, and see what the author of the object template \n",
    "# (the class) has to say\n",
    "print(type(iris_db))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Depending on your environment, the result should be something like\n",
    "# <class 'sklearn.utils.Bunch'>\n",
    "# \"Bunch\" is the name of the class, living in the \"utils\" sub-module\n",
    "# which, in turn, is in the \"sklearn\" library.\n",
    "\n",
    "# Now let's get some help of the class. (most popular libraries\n",
    "# are well documented)\n",
    "iris_db?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# As the doc-string doesn't provide much information of this object.\n",
    "# let's try method\n",
    "# 2. duck-typing: check how the object \"quacks\" and \"walks\"\n",
    "dir(iris_db) # dir() lists methods / attributes of an object.\n",
    "\n",
    "# you may find \"DESCR\" to be useful, try to print it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# The most interesting parts of the dataset object are\n",
    "# `data` and `target` of course. Let's check `data`.\n",
    "type(iris_db.data), type(iris_db.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# We are familiar with numpy arrays. Perform some standard checks\n",
    "print(iris_db.data.shape, iris_db.target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# This looks like 150 data samples with 150 corresponding labels.\n",
    "print(iris_db.data[:10], iris_db.target[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# END of Data Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# just to save typing\n",
    "X, y = iris_db.data, iris_db.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Simplification: Class Setosa vs Versicolor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Without losing generality, we take a further simplification step by considering only two flower classes. Rather than identifying versicolor from all iris flowers, we distinguish Versicolor from Setosa. Now we take the samples belonging to the first two classes (`target==0` for setosa and `target==1` for versicolor). \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### A note on programming $^{ProgSkill}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "(Skip such sections/comments on programming skills (marked with $^{ProgSkill}$) if you feel comfortable to).\n",
    "\n",
    "The hard part is to express an idea in clear and specific terms. It is relatively easy to translate such expressions into any particular programming language. For example, consider the task to take the samples belonging to the first 2 classes and make a subset of the data set.\n",
    "\n",
    "To specify a subset,  we consider the conditions each individual element in the subset should satisfy:  for our task, that is “the target value of this sample is 0 or the target value of this sample is 1”. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# So the idea becomes: \n",
    "# for each training sample, (let us identify the training sample \n",
    "# using an index i), \n",
    "# - include the data[i] and the target[i] in the subset, \n",
    "#   if target[i] is 0 or 1.\n",
    "\n",
    "# translating the idea into a program\n",
    "X_sub = []\n",
    "y_sub = []\n",
    "for i in range(len(iris_db.data)): \n",
    "    # [EXERCISE] What does range(len(...)) do \n",
    "    # for an ensemble object? \n",
    "    if iris_db.target[i] == 0 or iris_db.target[i] == 1:\n",
    "        X_sub.append(iris_db.data[i])\n",
    "        y_sub.append(iris_db.target[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Check what we have obtained.\n",
    "print(X_sub[0:5])\n",
    "print(y_sub[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# We can organise X_sub and y_sub as numpy arrays.\n",
    "# (so we can access the elements more easily)\n",
    "X_sub = np.array(X_sub)\n",
    "y_sub = np.array(y_sub)\n",
    "print(X_sub[0:5])\n",
    "print(y_sub[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Python allows us to express the idea more directly: the for-loop can be constructed using every pair of X and y in the training dataset, without introducing an index. Try the following code block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Translating the idea into a program\n",
    "X_sub = []\n",
    "y_sub = []\n",
    "for x_, y_ in zip(iris_db.data, iris_db.target): \n",
    "    # [EXERCISE] Print the iteration variables in a zipped list.\n",
    "    # I.e. construct two lists L1 and L2, and make a for loop\n",
    "    # over \"a, b in zip(L1, L2)\", check the values of a and b.\n",
    "    if y_ == 0 or y_ == 1:\n",
    "        X_sub.append(x_)\n",
    "        y_sub.append(y_)\n",
    "X_sub = np.array(X_sub)\n",
    "y_sub = np.array(y_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# (*) We can use more \"descriptive\", less \"instructive\" construction\n",
    "X_sub = np.array([x_ for x_, y_ in zip(iris_db.data, iris_db.target)\n",
    "                  if y_ == 0 or y_ == 1])\n",
    "y_sub = np.array([y_ for x_, y_ in zip(iris_db.data, iris_db.target)\n",
    "                  if y_ == 0 or y_ == 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Fast selection using numpy array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Numpy allows to use boolean conditions as indexes for arrays. Check the [document][bool-ind] for more details.\n",
    "\n",
    "[bool-ind]:https://docs.scipy.org/doc/numpy-1.13.0/user/basics.indexing.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X_sub = X[(y==0) + (y==1)] # \"+\" for boolean OR\n",
    "y_sub = y[(y==0) + (y==1)]\n",
    "\n",
    "# Consolidate following reference to the data\n",
    "X = X_sub\n",
    "y = y_sub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Take the first 2 attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X = X[:, :2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Discretise the attributes into 50 \"ticks\" $^{ProgSkill}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We align training samples in the data into grids in X-space. This is mostly for the consistency of demonstration and practice purposes. You can skip this section and the experimental results below would be approximately the same.\n",
    "\n",
    "Simply speaking, it works like as if you ticking the “align to grid” option when organising the icons on your desktop screen. The samples will be “snapped” to the points in a grid in the 2D X-space. This makes the training data part of the “complete X-space points” we will use for demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "quant_num = IRIS_EXPERIMENT_X_SPACE_QUANTISATION_BIN_NUM\n",
    "\n",
    "# A grid of cells to store the y-values in each area in\n",
    "# X-space. \n",
    "data_bins = [[[] for a1 in range(quant_num)]\n",
    "             for a2 in range(quant_num)]\n",
    "\n",
    "# Decide which bin each training sample belongs to\n",
    "\n",
    "# 1. We want the smallest value of an attribute to be stored in bin[0]\n",
    "#    and largest in bin[49] (say, for 50 bins)\n",
    "attrib0_min_value = X[:,0].min()\n",
    "attrib0_max_value = X[:,0].max() # perform 0-1 normalisation on attribute 0\n",
    "X[:, 0] = (X[:, 0] - attrib0_min_value) / (attrib0_max_value - attrib0_min_value)\n",
    "\n",
    "attrib1_min_value = X[:,1].min()\n",
    "attrib1_max_value = X[:,1].max()\n",
    "X[:, 1] = (X[:, 1] - attrib1_min_value) / (attrib1_max_value - attrib1_min_value)\n",
    "\n",
    "for x_, y_ in zip(X, y):\n",
    "    attrib0_bin_index = int(round(x_[0] * (quant_num - 1)))\n",
    "    attrib1_bin_index = int(round(x_[1] * (quant_num - 1)))\n",
    "    data_bins[attrib0_bin_index][attrib1_bin_index].append(y_)\n",
    "    # print(attrib0_bin_index, attrib1_bin_index)\n",
    "    \n",
    "# now we can arrange the data in a grid\n",
    "X_quant = []\n",
    "y_quant = []\n",
    "for grid_row in range(quant_num):\n",
    "    for grid_col in range(quant_num):\n",
    "        this_bin = data_bins[grid_row][grid_col]\n",
    "        if len(this_bin) > 0:\n",
    "                X_quant.append((grid_row, grid_col))\n",
    "                vote = int(round(np.mean(this_bin))) # if more than one y-value\n",
    "                # has been allocated to this cell (small area in X-space),\n",
    "                # we let them vote and take the majority.\n",
    "                y_quant.append(vote)\n",
    "                \n",
    "X_quant = np.array(X_quant) / quant_num # (0..1)\n",
    "y_quant = np.array(y_quant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X = X_quant\n",
    "y = y_quant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Data preprocessing summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Put all preprocessing steps together. Run the two cells below for proprocessed data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     10
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def preproc_data_for_complexity_experiment_quick(iris_db):\n",
    "    \n",
    "    X, y = iris_db.data, iris_db.target\n",
    "    X = X[(y==0) + (y==1)][:, :2]\n",
    "    y = y[(y==0) + (y==1)]\n",
    "    \n",
    "    # normalise to 0-1\n",
    "    X = (X - X.min(axis=0)) / (X.max(axis=0) - X.min(axis=0))\n",
    "    return X, y\n",
    "\n",
    "def quantise_data(X, y, quant_num):\n",
    "    \n",
    "    # merge different y-values in the same cell                 \n",
    "    X = np.round(X  * (quant_num - 1)).astype(np.int)\n",
    "    data_bins = [[[] for a1 in range(quant_num)]\n",
    "             for a2 in range(quant_num)]\n",
    "    for (a1_, a2_), y_ in zip(X, y):\n",
    "        data_bins[a1_][a2_].append(y_)\n",
    "    X_quant = []\n",
    "    y_quant = []\n",
    "    for grid_row in range(quant_num):\n",
    "        for grid_col in range(quant_num):\n",
    "            this_bin = data_bins[grid_row][grid_col]\n",
    "            if len(this_bin) > 0:\n",
    "                    X_quant.append((grid_row, grid_col))\n",
    "                    vote = int(round(np.mean(this_bin))) # if more than one y-value\n",
    "                    # has been allocated to this cell (small area in X-space),\n",
    "                    # we let them vote and take the majority.\n",
    "                    y_quant.append(vote)\n",
    "    X = np.array(X_quant) / quant_num # (0..1)\n",
    "    y = np.array(y_quant)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X, y = preproc_data_for_complexity_experiment_quick(iris_db)\n",
    "# you can comment out the following statement if not wanting quantisation\n",
    "X, y = quantise_data(X, y, IRIS_EXPERIMENT_X_SPACE_QUANTISATION_BIN_NUM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Visualise the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We will make a figure showing the training data we had prepared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     31
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "layout = go.Layout(\n",
    "    xaxis=go.layout.XAxis(\n",
    "        range=[0, 1],\n",
    "        showgrid=True,\n",
    "        zeroline=True,\n",
    "        showline=True,\n",
    "        gridcolor='#bdbdbd',\n",
    "        gridwidth=1,\n",
    "        zerolinecolor='#969696',\n",
    "        zerolinewidth=2,\n",
    "        linecolor='#636363',\n",
    "        linewidth=2,\n",
    "        mirror=True,\n",
    "    ),\n",
    "    yaxis=go.layout.YAxis(\n",
    "        range=[0, 1],\n",
    "        showgrid=True,\n",
    "        zeroline=True,\n",
    "        showline=True,\n",
    "        gridcolor='#bdbdbd',\n",
    "        gridwidth=1,\n",
    "        zerolinecolor='#969696',\n",
    "        zerolinewidth=2,\n",
    "        linecolor='#636363',\n",
    "        linewidth=2,\n",
    "        mirror=True,\n",
    "   ),\n",
    "   height=600,\n",
    "   width=600,\n",
    ")\n",
    "\n",
    "fig = go.Figure(\n",
    "    data=[\n",
    "        go.Scatter(\n",
    "            x=all_x[:, 0], \n",
    "            y=all_x[:, 1], \n",
    "            marker_color=\"rgba(0.7, 0.7, 0.7, 0.3)\",\n",
    "            marker_size=6,\n",
    "            marker_line_width=2,\n",
    "            mode=\"markers\",\n",
    "            name=\"'All' X Space Samples\"),\n",
    "        go.Scatter(\n",
    "            x=X[:, 0], \n",
    "            y=X[:, 1], \n",
    "            marker_color=y,\n",
    "            marker_size=12,\n",
    "            marker_line_width=2,\n",
    "            mode=\"markers\",\n",
    "            name=\"Dataset Samples\"), \n",
    "    ],\n",
    "    layout=layout,\n",
    "    layout_title_text=\"Quantized Simplified Iris Data\"\n",
    ")\n",
    "if COLAB:\n",
    "    fig.show(renderer=\"colab\")\n",
    "else:\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Test model complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We use decision trees for example, where different settings of the maximum depths represent different complexity of the models.  I.e. a decision tree that can build many levels of nodes is capable of fit more varieties in the training data,  while a decision tree with only few levels can only make simple splits in the data space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# This is a replica, FYI. The libraries have been imported above.\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "To demonstrate different challenges posed by the random training set during perform machine learning, we set up two conditions:\n",
    "\n",
    "    i) the number of training data samples is small, for example, we can set it as shown below.\n",
    "    \n",
    "    ii) there is noise in the training samples, so simply fit the training samples to high fidelity won’t work very well.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train_num = 40\n",
    "noise = 0.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "complex_E_in = []\n",
    "complex_E_out = [] # we cannot compute this, as we don't have the true concept \n",
    "# of Iris data. We can make one up in a later version for experiment. But for now, \n",
    "# let us use the evaluation on held-out test data instead. See below complex_E_test.\n",
    "complex_E_test = []\n",
    "simple_E_in = []\n",
    "simple_E_test = []\n",
    "\n",
    "# We perform multiple rounds of experiments to test the statistics of \n",
    "# E_in and E_test for different models / experiment settings.\n",
    "\n",
    "# Randomly add noise to the data.\n",
    "rng = np.random.RandomState(42)\n",
    "noisy_y = np.array([\n",
    "    y_ if rng.rand() > noise else (1-y_)\n",
    "    for y_ in y\n",
    "])\n",
    "for random_seed in range(500):\n",
    "    # Randomly split training/test data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, noisy_y, train_size=train_num, test_size=len(y)-train_num,\n",
    "        stratify=noisy_y,\n",
    "        random_state=random_seed)\n",
    "    \n",
    "    # Fit a complex model\n",
    "    dt_complex = DecisionTreeClassifier(max_depth=100)\n",
    "    dt_complex.fit(X_train, y_train)\n",
    "    \n",
    "    # Record training and test error\n",
    "    pred_on_X_train = dt_complex.predict(X_train)\n",
    "    E_in = np.sum(pred_on_X_train != y_train) / len(y_train) # error rate\n",
    "    complex_E_in.append(E_in)\n",
    "    pred_on_X_test = dt_complex.predict(X_test)\n",
    "    E_test = np.sum(pred_on_X_test != y_test) / len(y_test) # error rate\n",
    "    complex_E_test.append(E_test)\n",
    "    \n",
    "    # Fit a simple model\n",
    "    dt_simple = DecisionTreeClassifier(max_depth=2)\n",
    "    dt_simple.fit(X_train, y_train)\n",
    "    \n",
    "    # Record training and test error\n",
    "    pred_on_X_train = dt_simple.predict(X_train)\n",
    "    E_in = np.sum(pred_on_X_train != y_train) / len(y_train) # error rate\n",
    "    simple_E_in.append(E_in)\n",
    "    pred_on_X_test = dt_simple.predict(X_test)\n",
    "    E_test = np.sum(pred_on_X_test != y_test) / len(y_test) # error rate\n",
    "    simple_E_test.append(E_test)\n",
    "\n",
    "complex_E_in = np.array(complex_E_in)\n",
    "complex_E_test = np.array(complex_E_test)\n",
    "simple_E_in = np.array(simple_E_in)\n",
    "simple_E_test = np.array( simple_E_test)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(\"Complex Tree average E_in mean {:.3f}, std {:.3f}\"\n",
    "      .format(complex_E_in.mean(), complex_E_in.std()))\n",
    "print(\"Complex Tree average E_out mean {:.3f}, std {:.3f}\"\n",
    "      .format(complex_E_test.mean(), complex_E_test.std()))\n",
    "abs_diff = np.abs(complex_E_in - complex_E_test)\n",
    "print(\"Complex Tree average |E_out - E_in| mean {:.3f}, std {:.3f}\"\n",
    "      .format(abs_diff.mean(), abs_diff.std()))\n",
    "\n",
    "print(\"Simple Tree average E_in mean {:.3f}, std {:.3f}\"\n",
    "      .format(simple_E_in.mean(), simple_E_in.std()))\n",
    "print(\"Simple Tree average E_out mean {:.3f}, std {:.3f}\"\n",
    "      .format(simple_E_test.mean(), simple_E_test.std()))\n",
    "abs_diff = np.abs(simple_E_in - simple_E_test)\n",
    "print(\"Simple Tree average |E_out - E_in| mean {:.3f}, std {:.3f}\"\n",
    "      .format(abs_diff.mean(), abs_diff.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "__Experiment Records__\n",
    "\n",
    "<span style=\"color:green\">__EXERCISE__</span>\n",
    "\n",
    "Please try different experiment settings. Record your findings in the table below (double-click here to edit this cell). Please take notes for the following two topics:\n",
    "1. Why you had chosen the experiment configurations? Why you thought those numbers are worthy exploration?\n",
    "2. What did you expect to find for the parameters *BEFORE* you run the experiments?\n",
    "3. Do the outcomes match your expectation? Explain possible reasons of matching / mismatching. \n",
    "\n",
    "| $N$ | $\\eta$ | $E_{in}^C $  | $E_{test}^C$ | $D^C$  | $E_{in}^S$  | $E_{test}^S$ | $D^S$  | \n",
    "|---|---|---|---|---|---|---|---|\n",
    "| 40 | 0.15 | 0.000  |  0.413  | 0.413   | 0.202   | 0.321   | 0.124 |\n",
    "\n",
    "- $N$: number of training samples\n",
    "- $\\eta$: noise level (probability that in a training sample $(x, y)$, $y$ happens to be the _incorrect_ label for $x$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# 3 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Environment and constant preparation\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "try:\n",
    "    import plotly.graph_objects as go\n",
    "except:\n",
    "    !pip install plotly==4.1.0\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.datasets as cvdata\n",
    "import torchvision.transforms as transforms\n",
    "from pathlib import Path\n",
    "import torch\n",
    "\n",
    "DATA_FOLDER = Path(\"./data\").absolute()\n",
    "DATA_FOLDER.mkdir(parents=True, exist_ok=True)\n",
    "DATA_FOLDER = str(DATA_FOLDER)\n",
    "COLAB = False\n",
    "IRIS_EXPERIMENT_X_SPACE_QUANTISATION_BIN_NUM = 50 # self-interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## 3.1 Preparing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We use data from CIFAR object dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Downloading and Loading Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Download and make dataset\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "cifar_trainset = cvdata.CIFAR10(\n",
    "    root=DATA_FOLDER, train=True,\n",
    "    download=True, transform=transform)\n",
    "\n",
    "cifar_all_trainloader = torch.utils.data.DataLoader(\n",
    "    cifar_trainset, batch_size=64,\n",
    "    shuffle=False, num_workers=6)\n",
    "\n",
    "# take two classes for a subset\n",
    "cifar_classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "# collect samples in an overall array\n",
    "images = []\n",
    "labels = []\n",
    "indexes = []\n",
    "for i, (x, y) in enumerate(cifar_trainset):\n",
    "    if cifar_classes[y] in [\"plane\", \"bird\"]:\n",
    "        images.append(x)\n",
    "        twoclass_label = 1 if cifar_classes[y] == \"plane\" else -1\n",
    "        labels.append(twoclass_label)\n",
    "        indexes.append(i)\n",
    "        \n",
    "X = torch.stack(images).numpy()\n",
    "y = np.array(labels)\n",
    "indexes = np.array(indexes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Preprocessing (Doing Features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     1,
     4,
     7,
     21,
     60,
     63
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Defining processing functions\n",
    "def feature_maker_overall_green(X):\n",
    "    return X[:, 1].reshape(X.shape[0], -1).sum(axis=1)\n",
    "\n",
    "def feature_maker_overall_blue(X):\n",
    "    return X[:, 2].reshape(X.shape[0], -1).sum(axis=1)\n",
    "\n",
    "def take_a_separable_subset(X, y, original_indexes):\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    lr = LogisticRegression()\n",
    "    lr.fit(X, y)\n",
    "    pred = lr.predict(X)\n",
    "    pred_prob = lr.predict_log_proba(X_train)\n",
    "    confident_ind = np.logical_or(pred_prob[:, 0] > np.log(0.55),\n",
    "                                  pred_prob[:, 1] > np.log(0.55))\n",
    "    ind = np.logical_and(confident_ind, pred==y)\n",
    "    X_simple = X[ind]\n",
    "    y_simple = y[ind]\n",
    "    original_indexes_simple = original_indexes[ind]\n",
    "    return X_simple, y_simple, original_indexes_simple\n",
    "\n",
    "def prepare_cifar_two_class_data(\n",
    "    X, y, original_indexes,\n",
    "    simple=False,\n",
    "    train_size=1000,\n",
    "    make_feature_1=feature_maker_overall_green, \n",
    "    make_feature_2=feature_maker_overall_blue):\n",
    "    feature1 = make_feature_1(X)\n",
    "    feature2 = make_feature_2(X)\n",
    "    X = np.stack([feature1, feature2], axis=1)\n",
    "    # normalise to -1 to +1\n",
    "    X -= X.min(axis=0)\n",
    "    X /= X.max(axis=0)\n",
    "    X -= 0.5\n",
    "    X *= 2.0\n",
    "    \n",
    "    X_train, X_test, y_train, y_test, ind_train, ind_test = \\\n",
    "        train_test_split(X, y, original_indexes, \n",
    "                         train_size=train_size, test_size=len(y)-train_size)\n",
    "    return X_train, X_test, y_train, y_test, ind_train, ind_test\n",
    "\n",
    "def prepare_cifar_two_class_separable_data(\n",
    "    X, y, original_indexes, train_size=100):\n",
    "    \n",
    "    X_simple, y_simple, indexes_simple = \\\n",
    "        take_a_separable_subset(X, y, original_indexes)\n",
    "    \n",
    "    X_train_simple, X_test_simple, \\\n",
    "    y_train_simple, y_test_simple, \\\n",
    "    ind_train_simple, ind_test_simple = \\\n",
    "        train_test_split(X_simple, y_simple, indexes_simple, \n",
    "                         train_size=train_size, test_size=len(y_simple)-100)\n",
    "    return X_train_simple, X_test_simple, \\\n",
    "        y_train_simple, y_test_simple, \\\n",
    "        ind_train_simple, ind_test_simple\n",
    "    \n",
    "X_train, X_test, y_train, y_test, ind_train, ind_test = \\\n",
    "    prepare_cifar_two_class_data(X, y, indexes)\n",
    "X_train_simple, X_test_simple, \\\n",
    "y_train_simple, y_test_simple, \\\n",
    "ind_train_simple, ind_test_simple = prepare_cifar_two_class_separable_data(\n",
    "    X_train, y_train, ind_train)\n",
    "\n",
    "def quick_separable_train_sample(n=100):\n",
    "    X_train_simple, X_test_simple, \\\n",
    "    y_train_simple, y_test_simple, \\\n",
    "    ind_train_simple, ind_test_simple = prepare_cifar_two_class_separable_data(\n",
    "        X_train, y_train, ind_train, train_size=n)\n",
    "    \n",
    "    return X_train_simple, y_train_simple, ind_train_simple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Visualisation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     1,
     7
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Defining vis-functions\n",
    "def show_cifar_image(img_id):\n",
    "    npimg = ((cifar_trainset[img_id][0].detach().numpy() + 1.0) \\\n",
    "        * 128).astype(np.uint8).transpose((1, 2, 0))\n",
    "    # plt.imshow(npimg, interpolation='nearest') # for larger view\n",
    "    return Image.fromarray(npimg), npimg\n",
    "\n",
    "def encode_sample_image(index):\n",
    "    \"\"\"\n",
    "    Generate the resource url to display an image in a webpage.\n",
    "    This is not used in notebooks. But you can take the function\n",
    "    in a standalone Python program as a web-server to visual data models.\n",
    "    \"\"\"\n",
    "    import base64\n",
    "    from io import BytesIO\n",
    "\n",
    "    pil_img = Image.fromarray(((cifar_trainset[index][0].numpy()\n",
    "                                .transpose([1, 2, 0]) + 1.0) * 128).astype(np.uint8))\n",
    "    buff = BytesIO()\n",
    "    pil_img.save(buff, format=\"JPEG\")\n",
    "    new_image_string = base64.b64encode(buff.getvalue()).decode(\"utf-8\")\n",
    "    # print(new_image_string[:100])\n",
    "    return \"\"\"<img src=\"data:image/png;base64,\"\"\" \\\n",
    "        + \"\"\"\"></img>\"\"\"\n",
    "\n",
    "def show_perceptron_model(model, X_train, y_train, indexes_train=[]):\n",
    "    layout = go.Layout(\n",
    "        xaxis=go.layout.XAxis(\n",
    "            range=[-1, 1],\n",
    "            showgrid=True,\n",
    "            zeroline=True,\n",
    "            showline=True,\n",
    "            gridcolor='#bdbdbd',\n",
    "            gridwidth=1,\n",
    "            zerolinecolor='#969696',\n",
    "            zerolinewidth=2,\n",
    "            linecolor='#636363',\n",
    "            linewidth=2,\n",
    "            mirror=True,\n",
    "        ),\n",
    "        yaxis=go.layout.YAxis(\n",
    "            range=[-1, 1],\n",
    "            showgrid=True,\n",
    "            zeroline=True,\n",
    "            showline=True,\n",
    "            gridcolor='#bdbdbd',\n",
    "            gridwidth=1,\n",
    "            zerolinecolor='#969696',\n",
    "            zerolinewidth=2,\n",
    "            linecolor='#636363',\n",
    "            linewidth=2,\n",
    "            mirror=True,\n",
    "       ),\n",
    "       height=600,\n",
    "       width=600,\n",
    "    )\n",
    "\n",
    "    # visualise perceptron model on a grid\n",
    "    x_grid, y_grid = np.meshgrid(np.arange(-1, 1.01, 0.05), np.arange(-1, 1.01, 0.05))\n",
    "    grid_X = np.stack(( x_grid.flatten(), y_grid.flatten()) ).T\n",
    "    grid_pred = model.predict(grid_X)\n",
    "    train_pred = model.predict(X_train)\n",
    "    train_error_num = (train_pred.astype(np.int) != y_train.astype(np.int)).sum()\n",
    "    E_in = train_error_num / len(y_train)\n",
    "\n",
    "    scatter_grid = go.Scatter(\n",
    "        x=grid_X[:, 0], y=grid_X[:, 1], \n",
    "        marker=dict(\n",
    "            size=6,\n",
    "            cmax=1,\n",
    "            cmin=-1,\n",
    "            line_width=1,\n",
    "            color=grid_pred,\n",
    "            colorscale=\"Cividis\",\n",
    "            symbol=\"square\",\n",
    "            opacity=0.5\n",
    "        ),\n",
    "        mode=\"markers\",\n",
    "    #     colorscale=,\n",
    "        name=\"'All' X Space Samples\",\n",
    "        hoverinfo=\"none\")\n",
    "    \n",
    "    contour_grid = go.Contour(\n",
    "        z=grid_pred,\n",
    "        x=grid_X[:, 0], # horizontal axis\n",
    "        y=grid_X[:, 1], # vertical axis\n",
    "        hoverinfo=\"none\",\n",
    "        colorscale=\"Cividis\",\n",
    "        showscale=False\n",
    "    )\n",
    "    \n",
    "    if len(indexes_train) == 0:\n",
    "        scatter_train_text = [\"X:({:.02f}, {:.02f})<br>y:{}, pred:{}\".format(x0, x1, int(y), int(p)) \n",
    "              for (x0, x1), y, p in zip(X_train, y_train, train_pred)] \n",
    "    else:\n",
    "        scatter_train_text = [\"X:({:.02f}, {:.02f})<br>y:{}, pred:{}, ImgID {:d}\"\\\n",
    "                              .format(x0, x1, int(y), int(p), i) \n",
    "                              for (x0, x1), y, p, i in zip(X_train, y_train, train_pred, indexes_train)] \n",
    "        \n",
    "\n",
    "    scatter_train = go.Scatter(\n",
    "        x=X_train[:, 0], y=X_train[:, 1],\n",
    "         marker=dict(\n",
    "             size=12,\n",
    "             cmax=1,\n",
    "             cmin=-1,\n",
    "             color=y_train,\n",
    "             colorscale=\"Cividis\",\n",
    "             line=dict(\n",
    "                 width=2,\n",
    "                 color=[\"green\" if prediction == ground_truth else \"red\"\n",
    "                        for prediction, ground_truth in zip(train_pred, y_train)]\n",
    "             )\n",
    "\n",
    "        ),\n",
    "        mode=\"markers\",\n",
    "        name=\"Dataset Samples\",\n",
    "        text=scatter_train_text,\n",
    "        hoverinfo=\"text\")\n",
    "\n",
    "    fig = go.Figure(\n",
    "        data=[\n",
    "            contour_grid, scatter_train\n",
    "        ],\n",
    "        layout=layout,\n",
    "        layout_title_text=\"Two Object Class 2D Data<br>#.errors={:d}, E_in={:.3f}\"\\\n",
    "            .format(train_error_num, E_in)\n",
    "    )\n",
    "    if COLAB:\n",
    "        fig.show(renderer=\"colab\")\n",
    "    else:\n",
    "        fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## 3.2 Perceptron Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The prediction function. A perceptron consists of the weights associated to all data attributes and a bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class MyPerceptron2D:\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        There are three parameters for a perceptron working on 2D data.\n",
    "        w0, w1: the coefficients of the first and second attribute x0 and x1, respectively\n",
    "        b: the bias\n",
    "        \"\"\"\n",
    "        self.w0 = 1.0\n",
    "        self.w1 = 0\n",
    "        self.b = 0\n",
    "        \n",
    "    def predict(self, x):\n",
    "        \"\"\"\n",
    "        Given a data sample (x0, x1), the perceptron first computes the \n",
    "        \"pre-activation potential\" -- a term borrowed from biological neurons --\n",
    "        using simple linear function:\n",
    "        \n",
    "            pre-activation := w0 * x0 + w1 * x1 + bias\n",
    "        \n",
    "        Note this implementation we accept numpy array as input `x`, where \n",
    "        x is of the format \n",
    "        [[x0, x1] .. for sample-0\n",
    "         [x0, x1] .. for sample-1\n",
    "         [x0, x1] .. for sample-2\n",
    "         ...]\n",
    "         \n",
    "        it contains N samples, each of 2 attributes. As a Numpy array, x provides \n",
    "        convenient access to specific attributes of all samples. We can compute\n",
    "        the pre-activation values of N samples easily as shown in the code.\n",
    "        \n",
    "        The prediction is straightforward given the pre-activation values: it amounts to\n",
    "        determine if the pre-activation is above or below zero.\n",
    "        \"\"\"\n",
    "        prediction = x[:, 0] * self.w0 \\\n",
    "            + x[:, 1] * self.w1 \\\n",
    "            + self.b\n",
    "        \n",
    "        prediction[prediction > 0] = 1\n",
    "        prediction[prediction <= 0] = -1\n",
    "        return prediction\n",
    "    \n",
    "    \n",
    "    def update(self, dw0, dw1, db, verbose=False):\n",
    "        \"\"\"\n",
    "        Incrementally adjust the model parameters\n",
    "        \"\"\"\n",
    "        if verbose:\n",
    "            print(\"Old perceptron:\", self)\n",
    "        self.w0 += dw0\n",
    "        self.w1 += dw1\n",
    "        self.b += db\n",
    "        if verbose: \n",
    "            print(\"New perceptron:\", self)\n",
    "            \n",
    "    def set_param(self, w0, w1, b, verbose=False):\n",
    "        if verbose:\n",
    "            print(\"Old perceptron:\", self)\n",
    "        self.w0 = w0\n",
    "        self.w1 = w1\n",
    "        self.b = b\n",
    "        if verbose: \n",
    "            print(\"New perceptron:\", self)\n",
    "            \n",
    "            \n",
    "    def fit(self, X, y):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def __str__(self):\n",
    "        return \"W0:{:.2f}, W1:{:.2f}, b:{:.2f}\".format(self.w0, self.w1, self.b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In the cell below, we construct a perceptron model to perform classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X_easy, y_easy, ind_easy = quick_separable_train_sample(20) # take some easy, small samples for experiment\n",
    "first_perceptron = MyPerceptron2D()\n",
    "show_perceptron_model(first_perceptron, X_easy, y_easy, ind_easy)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Hover the cursor on a training sample, it will show the id of \n",
    "# the corresponding image. Use the ID here to show the image.\n",
    "im, imnp = show_cifar_image(43507)\n",
    "im"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## 3.3 Training Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### A Manual \"Training\" Scheme"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<span style=\"color:green\">__EXERCISE__</span>\n",
    "\n",
    "How the perceptron worked on the data? In the cell below, \n",
    "please attempt to modify the parameters of the perceptron model, so the prediction matches the data better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "first_perceptron.update(dw0=0, dw1=0, db=0.05, verbose=True)\n",
    "# first_perceptron.set_param(w0=-0.02, w1=0.08, b=.0, verbose=True)\n",
    "show_perceptron_model(first_perceptron, X_easy, y_easy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Training a Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The motiviation is as follows:\n",
    "\n",
    "- we need to modify the model parameters when our perceptron makes predictions disagree with the given $y$ for some training data samples.\n",
    "\n",
    "- Now consider an example: `[x0, x1, +1]`, since our perceptron predicted `-1` for `[x0, x1]` (otherwise, we won't consider this data sample now). That is to say, according to our perceptron model:\n",
    "\n",
    "    `a == w0*x0 + w1*x1 + b < 0`\n",
    "    \n",
    "It is natural to take measures to increase `a`. As shown intuitively above, we can update the model parameters to \"rotate\" the classification boundary, so let us consider `w0` and `w1` for now.\n",
    "\n",
    "> (If you feel uncomfortable about leaving `b` behind for now, please check [Chapter1.1 @ Equation 1.2][Abu-Mostafa et al. 2012] for a side note. The note is about treating the bias `b` as a special coefficient `w_b`. So we can bring `b` into the training framework below).\n",
    "\n",
    "- Given a certain step size, say 0.1 -- `dw = (dw0, dw1)` and  $\\sqrt{dw_0^2 + dw_1^2} = 0.1$ -- the most efficient way to let `a == w0*x0 + w1*x1` increase is to arrange `dw0` and `dw1` proportional to `x0` and `x1`.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{dw_0}{dw_1} = \\frac{dx_0}{dx_1}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "<span style=\"color:green\">__EXERCISE__</span>\n",
    "\n",
    "Sketch an illustration in a 2D plane, draw vectors of `x` and `w`, show why $\\frac{dw_0}{dw_1} = \\frac{dx_0}{dx_1}$ is most efficient given a fixed step size.\n",
    "\n",
    "\n",
    "[Abu-Mostafa et al. 2012]:http://amlbook.com/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<span style=\"color:green\">__EXERCISE (Main)__</span>\n",
    "\n",
    "Create a perceptron and examine its performance on the training dataset in the cell below (the same as done in the previous section). \n",
    "\n",
    "Then study the code block \"Individual Training Step\" below. Try to get a perceptron with complete fit to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "perceptron_on_train = MyPerceptron2D()\n",
    "show_perceptron_model(perceptron_on_train, X_easy, y_easy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Individual Training Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Study Single Step Training\n",
    "\n",
    "# Let the perceptron predict for X-samples and find where it makes mistakes\n",
    "prediction_on_X_easy = perceptron_on_train.predict(X_easy)\n",
    "E_in_index = list(np.nonzero(prediction_on_X_easy != y_easy)[0])\n",
    "print(\"Error-in @\", E_in_index)\n",
    "\n",
    "if len(E_in_index) > 0:\n",
    "    # Take one problematic sample \n",
    "    to_fix_index = E_in_index[0]\n",
    "    to_fix_sign = y_easy[to_fix_index]\n",
    "    dw = X_easy[to_fix_index]\n",
    "\n",
    "    # To normalise the change to our prescribed stepsize 0.1\n",
    "    stepsize = 0.1\n",
    "    dw = dw / np.linalg.norm(dw) * stepsize * to_fix_sign\n",
    "    print(\"Proposed adjustment of w {} (after normalisation), sign {}\"\n",
    "          .format(dw, to_fix_sign))\n",
    "\n",
    "    # Apply the update and show\n",
    "    perceptron_on_train.update(dw0=dw[0], dw1=dw[1], db=0, verbose=True)\n",
    "else:\n",
    "    print(\"Done training\")\n",
    "    \n",
    "show_perceptron_model(perceptron_on_train, X_easy, y_easy)\n",
    "\n",
    "# you may want to check individual samples (see the image) by using\n",
    "# `show_cifar_image` as above. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Training Procedure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<span style=\"color:green\">__EXERCISE (Main)__</span>\n",
    "\n",
    "1. Wrap the training code block with a loop to make a working training procedure.\n",
    "2. Copy-and-paste the definition of perceptron below, and provide the implementation of the `fit` method.\n",
    "3. Try `sklearn` perceptron implementation and train the model. Take a note on this common _interface_ of machine learning algorithm design."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Test sklearn Perceptron\n",
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "skperceptron = Perceptron()\n",
    "# Perform training here\n",
    "show_perceptron_model(skperceptron, X_easy, y_easy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# 4 Bias-variance Ballance and Bootstrapping (BBB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Environment and constant preparation\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "try:\n",
    "    import plotly.graph_objects as go\n",
    "    import plotly.express as px\n",
    "except:\n",
    "    !pip install plotly==4.1.0\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import scipy.stats as ss\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_gmm_samples(sample_num, norm_params, \n",
    "                    weights=None, rng=None):\n",
    "    \"\"\"\n",
    "    Generate samples from a Gaussian distribution\n",
    "    \"\"\"\n",
    "    # take samples in two steps:\n",
    "    # 1. a stream of indices from which to choose the component\n",
    "    norm_params = np.atleast_2d(np.array(norm_params))\n",
    "    component_num = norm_params.shape[0]\n",
    "    if weights is None:\n",
    "        weights = np.ones(component_num) / component_num\n",
    "    if rng is None:\n",
    "        rng = np.random.RandomState(42)\n",
    "    component_num = norm_params.shape[0]\n",
    "    mixture_idx = rng.choice(component_num, size=sample_num, \n",
    "                             replace=True, p=weights)\n",
    "    # 2. sample the corresponding gaussian\n",
    "\n",
    "    x_samples = np.array([\n",
    "        (rng.randn() + norm_params[i][0]) * norm_params[i][1]\n",
    "        for i in mixture_idx])\n",
    "    return x_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Visualisers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Bootstrap Sample Visualiser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     24,
     73
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def show_bootstrap_samples(\n",
    "    norm_params,\n",
    "    weights=None,\n",
    "    bootstrap_times_to_display=3,\n",
    "    show_sample=False,\n",
    "    show_bootstrap_sample=False,\n",
    "    estimator=np.median,\n",
    "    show_pdf_text=False,\n",
    "    rng=None,\n",
    "    sample_num=None\n",
    "):\n",
    "    norm_params = np.atleast_2d(np.array(norm_params))\n",
    "    component_num = norm_params.shape[0]\n",
    "    if weights is None:\n",
    "        weights = np.ones(component_num) / component_num\n",
    "    min_val = (norm_params[:, 0] - 3 * norm_params[:, 1]).min()\n",
    "    max_val = (norm_params[:, 0] + 3 * norm_params[:, 1]).max()\n",
    "    x_grid = np.linspace(min_val, max_val, 200)\n",
    "    mixture_pdf = np.zeros_like(x_grid)\n",
    "\n",
    "    for (l, s), w in zip(norm_params, weights):\n",
    "        mixture_pdf += ss.norm.pdf(x_grid, loc=l, scale=s) * w\n",
    "\n",
    "    \n",
    "    layout = go.Layout(\n",
    "        hovermode=\"closest\",\n",
    "        xaxis=go.layout.XAxis(\n",
    "            range=[min_val, max_val],\n",
    "            showgrid=False,\n",
    "            zeroline=False,\n",
    "            showline=True,\n",
    "            gridcolor='#bdbdbd',\n",
    "            gridwidth=1,\n",
    "            zerolinecolor='#969696',\n",
    "            zerolinewidth=2,\n",
    "            linecolor='#636363',\n",
    "            linewidth=2,\n",
    "            mirror=True,\n",
    "            showspikes=True\n",
    "        ),\n",
    "        yaxis=go.layout.YAxis(\n",
    "            range=[0, np.max(mixture_pdf)],\n",
    "            title=\"Probability\",\n",
    "            showgrid=False,\n",
    "            zeroline=False,\n",
    "            showline=True,\n",
    "            gridcolor='#bdbdbd',\n",
    "            gridwidth=1,\n",
    "            linecolor='#636363',\n",
    "            linewidth=2,\n",
    "            mirror=True,\n",
    "        ),\n",
    "        yaxis2=go.layout.YAxis(\n",
    "            range=[0, 10],\n",
    "            title=\"Samples\",\n",
    "            titlefont=dict(color=\"#ff7f0e\"),\n",
    "            tickfont=dict(color=\"#ff7f0e\"),\n",
    "            anchor=\"free\",\n",
    "            overlaying=\"y\",\n",
    "            side=\"right\",\n",
    "            position=1.00\n",
    "        ),\n",
    "        height=400,\n",
    "        width=600,)\n",
    "\n",
    "    if show_pdf_text:\n",
    "        pdf_text = [\"Prob(x={:.2f}) = {:.2f}\".format(x, y) \n",
    "                    for x, y in zip(x_grid, mixture_pdf)]\n",
    "        pdf_hoverinfo = \"text\"\n",
    "    else:\n",
    "        pdf_text = \"\"\n",
    "        pdf_hoverinfo = \"none\"\n",
    "        \n",
    "    pdf_scatter = go.Scatter(\n",
    "        x=x_grid, y=mixture_pdf, \n",
    "        marker=dict(\n",
    "            line_width=1,\n",
    "            color=\"blue\",\n",
    "            colorscale=\"Cividis\",\n",
    "            symbol=\"square\",\n",
    "            opacity=0.5\n",
    "        ),\n",
    "        mode=\"lines\",\n",
    "        fill='tozeroy',\n",
    "        name=\"MixNorm PDF\",\n",
    "        text=pdf_text,\n",
    "        hoverinfo=pdf_hoverinfo)\n",
    "\n",
    "    fig_data = [pdf_scatter, ]\n",
    "    \n",
    "   \n",
    "    if show_sample:\n",
    "        x_samples = get_gmm_samples(sample_num, norm_params, rng=rng)\n",
    "        sample_histogram = go.Histogram(\n",
    "            x=x_samples, \n",
    "            xbins=dict(start=min_val, end=max_val, size=0.05),\n",
    "            yaxis=\"y2\", \n",
    "            opacity=0.8,\n",
    "            marker=dict(color=\"orange\"),\n",
    "            name=\"Original Sample - Median:{:.2f}\"\n",
    "            .format(estimator(x_samples)))\n",
    "        \n",
    "        fig_data.append(sample_histogram)\n",
    "    \n",
    "    if show_sample and show_bootstrap_samples:\n",
    "        bootstrap_sample_histograms = []\n",
    "        for b in range(bootstrap_times_to_display):\n",
    "            bs = rng.choice(x_samples, size=sample_num, replace=True)\n",
    "            bh = go.Histogram(\n",
    "                x=bs,\n",
    "                xbins=dict(start=min_val, end=max_val, size=0.05),\n",
    "                yaxis=\"y2\", \n",
    "                opacity=0.8,\n",
    "                marker=dict(color=b, line=dict(color=\"blue\")),\n",
    "                name=\"Bootstrap Samples {:d} - Median:{:.2f}\"\n",
    "                .format(b, estimator(bs)))\n",
    "            bootstrap_sample_histograms.append(bh)\n",
    "        \n",
    "        fig_data += bootstrap_sample_histograms\n",
    "            \n",
    "\n",
    "    fig = go.Figure(\n",
    "            data=fig_data,\n",
    "            layout=layout,\n",
    "            layout_title_text=\"Probabilistic Density\",\n",
    "        )\n",
    "    if COLAB:\n",
    "        fig.show(renderer=\"colab\")\n",
    "    else:\n",
    "        fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Bootstrap vs Original Sample Estimation Visualiser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def show_bootstrap_original_comparison(results):\n",
    "    total_results = pd.concat(results, ignore_index=True)\n",
    "    fig_data = [go.Violin(\n",
    "            x=total_results[\"id\"],\n",
    "            y=total_results[\"original\"],\n",
    "            legendgroup='Original', scalegroup='Original', name='Original',\n",
    "            side='negative',\n",
    "            line_color='blue'), \n",
    "        go.Violin(\n",
    "            x=total_results[\"id\"],\n",
    "            y=total_results[\"bootstrap\"],\n",
    "            legendgroup='Bootstrap', scalegroup='Bootstrap', name='Bootstrap',\n",
    "            side='positive',\n",
    "            line_color='orange')]\n",
    "    fig=go.Figure(data=fig_data)\n",
    "    fig.update_traces(meanline_visible=True)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Bias Variance Visualiser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class VisualiseBiasVariance:\n",
    "    def __init__(self, h_factory, gnd):\n",
    "        x_a = np.arange(0, 1.001, 0.01)[:, np.newaxis]\n",
    "        y_a = gnd(x_a).squeeze()\n",
    "        self.ground_truth_scatter_trace = \\\n",
    "            go.Scatter(x=x_a.squeeze(), y=y_a, \n",
    "                       mode=\"lines\", \n",
    "                       name=\"Ground-truth\", \n",
    "                       line=dict(color='rgba(0.1, 0.0, 0.4)', width=5),\n",
    "                      )\n",
    "        \n",
    "        self.h_factory = h_factory\n",
    "        self.x_a = x_a\n",
    "        self.y_a = y_a\n",
    "        self.predictions = []\n",
    "        self.train_datasets = []\n",
    "        \n",
    "        \n",
    "    def add_experiment(self, x_trn, y_trn):\n",
    "        h = self.h_factory()\n",
    "        h.fit(x_trn, y_trn)\n",
    "        pred_a = h.predict(self.x_a)\n",
    "        self.train_datasets.append([x_trn, y_trn])\n",
    "        self.predictions.append(pred_a)\n",
    "        \n",
    "        \n",
    "    def show(self, show_train=False, show_var=False, show_mean=False):\n",
    "        layout = go.Layout(\n",
    "            xaxis=go.layout.XAxis(\n",
    "                range=[0, 1],\n",
    "                showgrid=True,\n",
    "                zeroline=True,\n",
    "                showline=True,\n",
    "                gridcolor='#bdbdbd',\n",
    "                gridwidth=1,\n",
    "                linecolor='#636363',\n",
    "                linewidth=2,\n",
    "                mirror=True,\n",
    "                showspikes=True,\n",
    "            ),\n",
    "            yaxis=go.layout.YAxis(\n",
    "                range=[-6, 6],\n",
    "                showgrid=True,\n",
    "                zeroline=True,\n",
    "                showline=True,\n",
    "                gridcolor='#bdbdbd',\n",
    "                gridwidth=1,\n",
    "                zerolinecolor='#969696',\n",
    "                zerolinewidth=2,\n",
    "                linecolor='#636363',\n",
    "                linewidth=2,\n",
    "                mirror=True,\n",
    "           ),\n",
    "           height=600,\n",
    "           width=600,\n",
    "        )\n",
    "        \n",
    "        fig_data = []\n",
    "        \n",
    "        x_a = self.x_a.squeeze()\n",
    "        \n",
    "        fig_data = [go.Scatter(\n",
    "            x=self.x_a.squeeze(), y=pred_a, mode=\"lines\", \n",
    "            line=dict(color='rgba(0.8, 0.6, 0.8, 0.5)', width=2),\n",
    "            showlegend=False,\n",
    "            text=[\"x:{:.2f}<br>pred:{:.2f}, gnd:{:.2f}, err:{:.2f}\"\\\n",
    "                  .format(x_, p_, y_, np.abs(p_ - y_)) \n",
    "                  for x_, p_, y_ in zip(x_a, pred_a, self.y_a)],\n",
    "            name=\"model{}\".format(ind),\n",
    "            hoverinfo=\"text+name\" if show_train else \"none\"\n",
    "        ) for ind, pred_a in enumerate(self.predictions)] # every 10 for less clutter\n",
    "        \n",
    "        if show_train:\n",
    "            fig_data += [\n",
    "                go.Scatter(\n",
    "                    x=x_trn.squeeze(), y=y_trn, mode=\"markers\", \n",
    "                    marker=dict(color='rgba(0.7, 0, 0.7, 0.5)', size=12),\n",
    "                    showlegend=False,\n",
    "                    text=[\"x:{:.2f}, gnd:{:.2f}\".format(x_, y_) \n",
    "                          for x_, y_ in zip(x_trn.squeeze(), y_trn)],\n",
    "                    name=\"model{}-trn\".format(ind),\n",
    "                    hoverinfo=\"text+name\" if show_train else \"none\"\n",
    "                ) for ind, (x_trn, y_trn) in enumerate(self.train_datasets)]\n",
    "            \n",
    "        fig_data += [self.ground_truth_scatter_trace]\n",
    "        \n",
    "        predictions = np.array(self.predictions)\n",
    "        mpred = predictions.mean(axis=0)\n",
    "        if show_var:\n",
    "            # Examine the mean square error by considering\n",
    "            # pred_mse = np.sqrt(((predictions - self.y_a)**2).mean(axis=0))            \n",
    "            pred_var = np.array(self.predictions).std(axis=0)\n",
    "            s1 = go.Scatter(x=self.x_a.squeeze(), y=mpred - pred_var,\n",
    "                            fill=None, mode='lines', line_color='indigo', \n",
    "                            text=[\"Var: {:.2f}\".format(v_) for v_ in pred_var],\n",
    "                            hoverinfo=\"x+text\",\n",
    "                            name=\"Variance\")\n",
    "            s2 = go.Scatter(x=self.x_a.squeeze(), y=mpred + pred_var,\n",
    "                            fill=\"tonexty\", mode='lines', line_color='indigo',\n",
    "                            showlegend=False,\n",
    "                            name=\"Variance\")\n",
    "            fig_data += [s1, s2]\n",
    "            # fig.add_trace(s2)\n",
    "            \n",
    "        if show_mean:\n",
    "            fig_data += [go.Scatter(x=self.x_a.squeeze(), y=mpred,\n",
    "                fill=None, mode='lines', line_color='blue', line_width=5,\n",
    "                text=[\"Mean: {:.2f}, Bias: {:.2f}\".format(m_, m_ - y_) \n",
    "                      for m_, y_ in zip(mpred, self.y_a)],\n",
    "                hoverinfo=\"text\",\n",
    "                name=\"Mean\")]\n",
    "            \n",
    "        fig = go.Figure(data=fig_data, layout=layout)\n",
    "        fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## 4.1 Introducing Bootstrapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Gaussian Mixture Probability Density"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We create a mixture of normal distribution to serve as the \"black-box ground-truth\" in our discussion: \n",
    "\n",
    "One normal distribution\n",
    "$$\n",
    "p(x) \\propto \\exp(\\frac{(x-\\textrm{mean})^2}{2\\textrm{stdvar}^2})\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "show_bootstrap_samples(norm_params=[0, 1],show_pdf_text=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Mixture of two normal distributions\n",
    "$$\n",
    "\\begin{align}\n",
    "p_1(x) & \\propto \\exp(\\frac{(x-\\textrm{mean}_1)^2}{2\\textrm{stdvar}_1^2}) \\\\\n",
    "p_2(x) & \\propto \\exp(\\frac{(x-\\textrm{mean}_1)^2}{2\\textrm{stdvar}_1^2}) \\\\\n",
    "p(x) & \\propto p_1(x) + p_2(x)\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "norm_params = np.array([[6, 1],\n",
    "                        [1, 1.3]])\n",
    "show_bootstrap_samples(norm_params, show_pdf_text=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## 4.2 A Simple Estimation Task  -- Estimating the Median"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let us consider a simple estimation task --  to estimate the median of the underlying data.  Note that as discussed in class this is a gross simplification of the process of \n",
    "1. building a hypotheses family and \n",
    "2. selecting a member from the family (training) and\n",
    "3. using the selected (trained) member to perform prediction at a test data point; and\n",
    "4. finally producing a number\n",
    "\n",
    "Now we consider a simple relationship between the  training data samples and the output number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Let's take a LOT of samples and estimate the \"Ground Truth\" median\n",
    "norm_params = np.array([[6, 1],\n",
    "                        [1, 1.3]]) # replica for ref\n",
    "x_samples = get_gmm_samples(1000000, norm_params)\n",
    "gnd_median = np.median(x_samples)\n",
    "print(gnd_median) # ~3.93"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Perform Estimation using One Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(0) # repeatable\n",
    "sample_num = 50\n",
    "x_samples = get_gmm_samples(sample_num, norm_params, rng=rng)\n",
    "sample_median = np.median(x_samples)\n",
    "print(\"Median estimated from one set of {} data samples: {:.2f}\"\n",
    "      .format(sample_num, np.median(x_samples)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<span style=\"color:#008000\">__EXERCISE__</span>\n",
    "Wrap the median computation with a loop, and compute multiple estimations (don't re-initialise the random generator). Report your observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sample_medians = []\n",
    "sample_sizes   = []\n",
    "for i in range(500):\n",
    "    for sn in [50, 100, 200]: # use different sample sizes\n",
    "        # -- BEGIN of EXERCISE\n",
    "        # Generate training sample and estimate median\n",
    "        # as shown in the example above, replace the\n",
    "        # random number below.\n",
    "        sample_median = rng.rand()\n",
    "        # -- END of EXERCISE\n",
    "        sample_medians.append(sample_median)\n",
    "        sample_sizes.append(sn)\n",
    "\n",
    "# Visualisation        \n",
    "df_median = pd.DataFrame(\n",
    "    data={\"sample_medians\":sample_medians,\n",
    "          \"sample_sizes\":sample_sizes})\n",
    "px.box(df_median,  x=\"sample_sizes\",\n",
    "       y=\"sample_medians\", )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Estimation using Bootstrap samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Let's take a few bootstrap samples from the original sample and compute the \n",
    "# bootstramp median\n",
    "\n",
    "# get bootstrap samples\n",
    "bootstrap_times = 80\n",
    "bootstrap_medians = []\n",
    "bootstrap_sample_generator = (\n",
    "    rng.choice(x_samples, size=sample_num, replace=True)\n",
    "    for _ in range(bootstrap_times))\n",
    "# (... for ...) makes a Python \"generator\", for _lazy evaluation_ \n",
    "# (google the term).\n",
    "\n",
    "bootstrap_medians.append(\n",
    "    [np.median(bootstrap_sample) \n",
    "     for bootstrap_sample in bootstrap_sample_generator])\n",
    "\n",
    "bootstrap_medians = np.array(bootstrap_medians)  \n",
    "print(\"Medians estimated from bootstrap samples have \"\n",
    "      \"mean {:.2f} and standard variance {:.2f}\".format(\n",
    "          bootstrap_medians.mean(),\n",
    "          bootstrap_medians.std(),\n",
    "      ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "It seems that we haven't gained much from bootstrapping,  but at least we now have an assessment on how reliable of the estimated median."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Explain Bootstrapping graphically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(42)\n",
    "show_bootstrap_samples(\n",
    "    norm_params, \n",
    "    show_sample=True,\n",
    "    sample_num=50,\n",
    "    rng=rng)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "It is clear from the demo that each bootstrap dataset is a re-sampled version of the original dataset. \n",
    "\n",
    "It is not surprise it does not provide a better estimation (even taking the mean of all bootstrap-estimations) than what we can compute from the original data. \n",
    "\n",
    "1. However, as discussed above, Bootstrap does give us an idea about how reliable the estimated median is.\n",
    "2. Although we have generated many bootstrapped datasets (`bootstrap_times = 80`, in the last case), and consider the _mean of the estimations (median)_ computed from the individual bootstrap datasets. But this is still ONE experiment. If we repeat the experiment multiple times we would see that the bootstrap estimation is indeed more reliable.\n",
    "\n",
    "\n",
    "Note: unlike the mean, median denies analytic equation. \n",
    "\n",
    "<span style=\"color:#008000\">__EXERCISE__</span>: By the way, how much we expect our estimation of the mean differs from the ground-truth mean?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "##### ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Consider the following problem: estimate the mean of a probabilistic distribution by using some samples. The estimation is expected to differ from the real mean -- It is just the variance of the distribution scaled down by the number of samples. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In the next experiment, we will verify (2) -- it sounds almost like free-lunch. But we do get the variance of the estimation reduced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Multiple Estimation Experiments (Bootstrap vs Original Sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In each of the experiment we first generate a data sample. From this data sample, we generate multiple versions of bootstrapping samples. We compare the estimation via bootstrapping and the original sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Reset all experiment results\n",
    "I_WANT_TO_START_FROM_SCRATCH = True\n",
    "if I_WANT_TO_START_FROM_SCRATCH:\n",
    "    results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#### CONFIGURATION OF EXPERIMENTS ####\n",
    "rng = np.random.RandomState(1) # repeatable\n",
    "# Parameters of the mixture components\n",
    "experiment_num = 200\n",
    "sample_num = 50 # the same\n",
    "bootstrap_times = 200 # in each experiment we take 80 \n",
    "                     # versions of BS-samples\n",
    "\n",
    "meta_experiment_name = \"Experiment Times {}<br>\" \\\n",
    "    \"Sample Num {}<br>\" \\\n",
    "    \"Bootstrap Times {}\".format(experiment_num, sample_num, bootstrap_times)\n",
    "\n",
    "#### RUNNING EXPERIMENTS ####\n",
    "norm_params = np.array([[6, 1],\n",
    "                        [1, 1.3]])\n",
    "sample_medians = []\n",
    "bootstrap_medians = []\n",
    "for experiment_id in range(experiment_num):\n",
    "    # get normal sample and compute the median\n",
    "    x_samples = get_gmm_samples(sample_num, norm_params, rng=rng)\n",
    "    \n",
    "    # get the bootstrap sample and compute the median\n",
    "    sample_medians.append(np.median(x_samples))\n",
    "    \n",
    "    # get bootstrap sample\n",
    "    bootstrap_sample_generator = (\n",
    "        rng.choice(x_samples, size=sample_num, replace=True)\n",
    "        for _ in range(bootstrap_times)) \n",
    "    bootstrap_medians.append([np.median(bootstrap_sample)\n",
    "                              for bootstrap_sample in bootstrap_sample_generator])\n",
    "    \n",
    "bootstrap_medians = np.array(bootstrap_medians)  \n",
    "bootstrap_median_final = bootstrap_medians.mean(axis=1) \n",
    "# take the mean of BS-median for each experiment\n",
    "sample_medians = np.array(sample_medians)\n",
    "\n",
    "print(bootstrap_median_final.mean(), bootstrap_median_final.std())\n",
    "print(sample_medians.mean(), sample_medians.std())\n",
    "results.append(\n",
    "    pd.DataFrame({\"bootstrap\": bootstrap_median_final, \n",
    "                  \"original\": sample_medians, \n",
    "                  \"id\": meta_experiment_name}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "You would notice the variance is reduced. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<span style=\"color:#008000\">__EXERCISE__</span>: Understand and change parameters in the above experiment:\n",
    "`bootstrap_times`, `sample_num`. And discuss when the re-sampling (bootstrapping) technique may be useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "show_bootstrap_original_comparison(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## 4.3 Bias and Variance in Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Bias and Variance Explained in a Simple Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "For example, the underlying truth is that $$ x\\rightarrow y: y = \\sin(3 \\pi x) + 1.5x $$\n",
    "And we are interested in the function on the range $x \\in [0, 1]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     3
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def true_f(x):\n",
    "    return np.sin(np.pi * x * 3)  + 1.5 * x\n",
    "\n",
    "def generate_training_dataset(n=10):\n",
    "    \"\"\"\n",
    "    Note: the training data is represented as an [n x 1] array. \n",
    "      The array has two dimensions. E.g. the dataset X: (0.1, 0.2, 0.3) \n",
    "      will be in the form of\n",
    "      [[0.1],\n",
    "       [0.2],\n",
    "       [0.3]]\n",
    "      This is to keep compatible with the convention that each *row* in\n",
    "      X represents an individual sample. It is convenient for comparing\n",
    "      our own method with those standard libraries. It is also useful, if \n",
    "      we are going to represent each data sample using richer features. \n",
    "      For example, we may choose to make a prediction model using not only \n",
    "      the x value, but also x^2. \n",
    "      \n",
    "      By taking x^2 another *attribute*, it is more straightforward to\n",
    "      represent the model using only two parameters (one for x and the\n",
    "      other for x^2). \n",
    "    \"\"\"\n",
    "    x = np.random.rand(n, 1)\n",
    "    y = true_f(x).squeeze()\n",
    "    return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Let's define two models\n",
    "from sklearn.linear_model import LinearRegression\n",
    "class ConstantPredictor:\n",
    "    def __init__(self):\n",
    "        self.pred = 0\n",
    "        \n",
    "    def fit(self, x, y):\n",
    "        self.pred = np.mean(y)\n",
    "        return self\n",
    "        \n",
    "    def predict(self, x):\n",
    "        return np.zeros(len(x)) + self.pred\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "vis = VisualiseBiasVariance(LinearRegression, true_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "vis.add_experiment(*generate_training_dataset(2))\n",
    "vis.show(show_train=True, show_mean=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<span style=\"color:#008000\">__EXERCISE__</span>\n",
    "Execute the cell above multiple times. Observe and understand how random training data affects individual predictions, say, for specific $x=0.6$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Let's repeat the experiment for multiple times\n",
    "for i in range(100):\n",
    "    vis.add_experiment(*generate_training_dataset(2))\n",
    "vis.show(show_var=True, show_mean=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## 4.4 Bootstrapping (Bagging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "TOTAL_TRAIN_NUM = 6 # 10 for counter-example and 20 for no effect\n",
    "BOOTSTRAP_TRAIN_NUM = TOTAL_TRAIN_NUM\n",
    "BOOTSTRAP_MODEL_NUM = TOTAL_TRAIN_NUM\n",
    "\n",
    "def feature3(x):\n",
    "    # feature engineering\n",
    "    return np.concatenate([x, np.exp(-(x-0.25)**2/18), np.exp(-(x-0.5)**2/18), np.exp(-(x-0.75)**2/18)], axis=1)\n",
    "    #return np.concatenate([x, np.exp(-(x-0.33)**2/18), np.exp(-(x-0.66)**2/18)], axis=1)\n",
    "\n",
    "class GeneralisedLinearRegression:\n",
    "    def __init__(self, feature_fn):\n",
    "        self.lin = LinearRegression()\n",
    "        self.feature_fn = feature_fn\n",
    "        \n",
    "    def fit(self, x, y):\n",
    "        self.lin.fit(self.feature_fn(x), y)\n",
    "        return self\n",
    "        \n",
    "    def predict(self, x):\n",
    "        return self.lin.predict(self.feature_fn(x))\n",
    "    \n",
    "def glm3_factory():    \n",
    "    return GeneralisedLinearRegression(feature3)\n",
    "\n",
    "class GeneralisedLinearRegressionBootstrap:\n",
    "    def __init__(self, feature_fn, bootstrap_train_num, bootstrap_num=10):\n",
    "        self.lin = [LinearRegression() for _ in range(bootstrap_num)]\n",
    "        self.feature_fn = feature_fn\n",
    "        self.bootstrap_num = bootstrap_num\n",
    "        self.bootstrap_train_num = bootstrap_train_num\n",
    "        \n",
    "    def fit(self, x, y):\n",
    "        for i in range(self.bootstrap_num):\n",
    "            ind = np.random.choice(len(x), self.bootstrap_train_num, replace=True)\n",
    "            self.lin[i].fit(self.feature_fn(x[ind]), y[ind])\n",
    "        return self\n",
    "        \n",
    "    def predict(self, x):\n",
    "        predicts = [m.predict(self.feature_fn(x)) for m in self.lin]\n",
    "        return np.array(predicts).mean(axis=0)\n",
    "    \n",
    "def glm3_bootstrapping_factory():    \n",
    "    return GeneralisedLinearRegressionBootstrap(\n",
    "        feature3, BOOTSTRAP_TRAIN_NUM, bootstrap_num=BOOTSTRAP_MODEL_NUM)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "vis = VisualiseBiasVariance(glm3_factory, true_f)\n",
    "for i in range(200):\n",
    "    vis.add_experiment(*generate_training_dataset(TOTAL_TRAIN_NUM))\n",
    "fig = vis.show(show_var=True, show_mean=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "vis = VisualiseBiasVariance(glm3_bootstrapping_factory, true_f)\n",
    "for i in range(200):\n",
    "    vis.add_experiment(*generate_training_dataset(TOTAL_TRAIN_NUM))\n",
    "fig = vis.show(show_var=True, show_mean=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "442px",
    "width": "387px"
   },
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
